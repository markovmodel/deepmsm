{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to construct a deep reversible Markov State Model with an attention mechanism and coarse-graining layers\n",
    "\n",
    "The training steps include:\n",
    "1. Pretraining of an ordinary VAMPnet (can be used to compare the results)\n",
    "2. Initiate the values for the paramter matrix $\\mathbf{S}$ and the reweighting vector $\\mathbf{u}$\n",
    "3. Training for $\\mathbf{u}$ and $\\mathbf{S}$\n",
    "4. Training for $\\boldsymbol{\\chi}$ and $\\mathbf{u}$ and $\\mathbf{S}$\n",
    "5. For estimating timescales only $\\mathbf{u}$ and $\\mathbf{S}$ have to be retrained\n",
    "6. For coarse-graining the parameters will be initialized by PCCA and then trained with the VAMP-E score \n",
    "\n",
    "The analysis consists of:\n",
    "1. Validation of the model via implied timescale and CK tests\n",
    "2. Estimating a network graph based on the estimated stationary distribution and transition matrix\n",
    "3. Analysis of the eigenfunctions\n",
    "4. Building a hierarchical model via the coarse-graining layers\n",
    "5. Saving structures with the help of $\\textit{mdtraj}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import matplotlib.gridspec as gridspec\n",
    "# mdtraj will be needed to save structures, pyemma will be needed for the PCCA initialization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for cuda supported device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where the data lies on the local machine\n",
    "# Needs to be adapted for own data!!!\n",
    "test_system = '2F4K'\n",
    "pdb_system = '2f4k_villin.pdb'\n",
    "\n",
    "root = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-0-protein/{0}-0-protein/{0}-0-protein'.format(test_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "output_all_files = np.load('/srv/public/andreas/data/desres/2f4k/villin_skip1.npy')\n",
    "traj_whole = output_all_files\n",
    "\n",
    "traj_data_points, input_size = traj_whole[0].shape\n",
    "# Skip data to make the data less correlated\n",
    "skip=25\n",
    "traj_whole_new = [traj_whole[0][::skip]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions, should be adapted for specific problems\n",
    "\n",
    "# number of output nodes/states of the MSM or Koopman model, therefore also nodes of chi\n",
    "# The list defines how the output will be coarse grained from first to last entry\n",
    "output_sizes = [4,3,2]\n",
    "# tau list for timescales estimation\n",
    "tau_list = [25,50,100,150,200]\n",
    "number_taus = len(tau_list)\n",
    "tau_list = np.array(tau_list)//skip\n",
    "\n",
    "# Tau, how much is the timeshift of the two datasets in the default training\n",
    "# tau for pretraining the vampnet usually smaller than the tau for the deepMSM\n",
    "tau = 5*2*25//skip # 5, 20\n",
    "tau_chi = 25//skip\n",
    "\n",
    "# Batch size for Stochastic Gradient descent\n",
    "batch_size = 10000\n",
    "batch_size_large = 20000\n",
    "\n",
    "# Which trajectory points percentage is used as training, validation, and rest for test\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "\n",
    "# How many hidden layers the network chi has\n",
    "network_depth = 4\n",
    "\n",
    "# Width of every layer of chi\n",
    "layer_width = 100\n",
    "\n",
    "# Learning rate used for the ADAM optimizer\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# create a list with the number of nodes for each layer\n",
    "nodes = [layer_width]*network_depth\n",
    "\n",
    "# epsilon for numerical inversion of correlation matrices\n",
    "epsilon = np.array(1e-7).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data generation\n",
    "\n",
    "def get_data_for_tau(traj_all, tau):\n",
    "    \n",
    "    for count, single_traj in enumerate(traj_all):\n",
    "        \n",
    "        \n",
    "    \n",
    "        if count == 0:\n",
    "            traj_ord = single_traj[:-tau]\n",
    "            traj_ord_lag = single_traj[tau:]\n",
    "        else:\n",
    "            traj_ord = np.concatenate((traj_ord, single_traj[:-tau]), axis=0)\n",
    "            traj_ord_lag = np.concatenate((traj_ord_lag, single_traj[tau:]), axis=0)\n",
    "    \n",
    "    length_data = traj_ord.shape[0]\n",
    "    \n",
    "    shuffle_indexes = np.arange(length_data)\n",
    "    np.random.shuffle(shuffle_indexes)\n",
    "\n",
    "    traj = traj_ord[shuffle_indexes]\n",
    "    traj_lag = traj_ord_lag[shuffle_indexes]\n",
    "    \n",
    "\n",
    "    length_train = int(np.floor((length_data) * train_ratio))\n",
    "    length_vali = int(np.floor((length_data) * valid_ratio))\n",
    "    \n",
    "    traj_data_train = traj[:length_train]\n",
    "    traj_data_train_lag = traj_lag[:length_train]\n",
    "    \n",
    "    end_vali = length_train+length_vali\n",
    "    traj_data_valid = traj[length_train:end_vali]\n",
    "    traj_data_valid_lag = traj_lag[length_train:end_vali]\n",
    "\n",
    "    \n",
    "    traj_data_test = traj[end_vali:]\n",
    "    traj_data_test_lag = traj_lag[end_vali:]\n",
    "    \n",
    "    # Input of the first network\n",
    "    X1_train = traj_data_train.astype('float32')\n",
    "    X2_train  = traj_data_train_lag.astype('float32')\n",
    "\n",
    "    # Input for validation\n",
    "    X1_vali = traj_data_valid.astype('float32')\n",
    "    X2_vali = traj_data_valid_lag.astype('float32')\n",
    "    \n",
    "    # Input for test\n",
    "    X1_test = traj_data_test.astype('float32')\n",
    "    X2_test = traj_data_test_lag.astype('float32')\n",
    "    \n",
    "    return X1_train, X2_train, X1_vali, X2_vali, X1_test, X2_test, length_train, length_vali, traj_ord, traj_ord_lag\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_koopman_op(trajs, tau, force_symmetric = False):\n",
    "    '''Estimates the koopman operator for a given trajectory at the lag time\n",
    "        specified. The formula for the estimation is:\n",
    "            K = C00 ^ -1 @ C01\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj: numpy array with size [traj_timesteps, traj_dimensions]\n",
    "        Trajectory described by the returned koopman operator\n",
    "\n",
    "    tau: int\n",
    "        Time shift at which the koopman operator is estimated\n",
    "        \n",
    "    force_symmetric: boolean, default = False\n",
    "        if true, calculates the symmetrized version of K instead\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    koopman_op: numpy array with shape [traj_dimensions, traj_dimensions]\n",
    "        Koopman operator estimated at timeshift tau\n",
    "\n",
    "    '''\n",
    "    # if tau larger 0, interpret trajs as either a list of trajectories\n",
    "    # or a single trajectory\n",
    "    # otherwise interpret trajs as a list of the data and time-lagged data\n",
    "    # possibly in random order\n",
    "    if tau > 0:\n",
    "        if type(trajs) == list:\n",
    "            traj = np.concatenate([t[:-tau] for t in trajs], axis = 0)\n",
    "            traj_lag = np.concatenate([t[tau:] for t in trajs], axis = 0)\n",
    "        else:\n",
    "            traj = trajs[:-tau]\n",
    "            traj_lag = trajs[tau:]\n",
    "    else:\n",
    "        traj = trajs[0]\n",
    "        traj_lag = trajs[1]\n",
    "        \n",
    "    koopman_op = np.eye(traj.shape[1])\n",
    "\n",
    "    c_0 = traj.T @ traj\n",
    "    c_tau = traj.T @ traj_lag\n",
    "    \n",
    "    # if you want to symmetrize the correlation matrices\n",
    "    if force_symmetric:\n",
    "        c_0 = c_0 + traj_lag.T @ traj_lag\n",
    "        c_tau = c_tau + traj_lag.T @ traj\n",
    "\n",
    "    eigv_all, eigvec_all = np.linalg.eig(c_0)\n",
    "    include = eigv_all > epsilon\n",
    "    eigv = eigv_all[include]\n",
    "    eigvec = eigvec_all[:,include]\n",
    "    c0_inv = eigvec @ np.diag(1/eigv) @ np.transpose(eigvec)\n",
    "\n",
    "    koopman_op = c0_inv @ c_tau\n",
    "\n",
    "    return koopman_op\n",
    "\n",
    "\n",
    "\n",
    "# utility function for plotting implied timescales\n",
    "# the hyperparameters allow for it to calculate errorbars and work from different input data types\n",
    "def get_its(data, lags, calculate_K = True, multiple_runs = False):\n",
    "    \n",
    "    def get_single_its(data):\n",
    "\n",
    "        if type(data) == list:\n",
    "            outputsize = data[0].shape[1]\n",
    "        else:\n",
    "            outputsize = data.shape[1]\n",
    "\n",
    "        single_its = np.zeros((outputsize-1, len(lags)))\n",
    "\n",
    "        for t, tau_lag in enumerate(lags):\n",
    "            if calculate_K:\n",
    "                koopman_op = estimate_koopman_op(data, tau_lag)\n",
    "            else:\n",
    "                koopman_op = data[t]\n",
    "            k_eigvals, k_eigvec = np.linalg.eig(np.real(koopman_op))\n",
    "            k_eigvals = np.sort(np.absolute(k_eigvals))\n",
    "            k_eigvals = k_eigvals[:-1]\n",
    "            single_its[:,t] = (-tau_lag / np.log(k_eigvals))\n",
    "\n",
    "        return np.array(single_its)\n",
    "\n",
    "\n",
    "    if not multiple_runs:\n",
    "\n",
    "        its = get_single_its(data)\n",
    "\n",
    "    else:\n",
    "\n",
    "        its = []\n",
    "        for data_run in data:\n",
    "            its.append(get_single_its(data_run))\n",
    "\n",
    "    return its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inv(x, ret_sqrt=False):\n",
    "    '''Utility function that returns the inverse of a matrix, with the\n",
    "    option to return the square root of the inverse matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: numpy array with shape [m,m]\n",
    "        matrix to be inverted\n",
    "\n",
    "    ret_sqrt: bool, optional, default = False\n",
    "        if True, the square root of the inverse matrix is returned instead\n",
    "    Returns\n",
    "    -------\n",
    "    x_inv: numpy array with shape [m,m]\n",
    "        inverse of the original matrix\n",
    "    '''\n",
    "\n",
    "    # Calculate eigvalues and eigvectors\n",
    "    eigval_all, eigvec_all = torch.symeig(x, eigenvectors=True)\n",
    "\n",
    "    # Filter out eigvalues below threshold and corresponding eigvectors\n",
    "    eig_th = torch.Tensor(epsilon)\n",
    "    index_eig = eigval_all > eig_th\n",
    "#     print(index_eig)\n",
    "    eigval = eigval_all[index_eig]\n",
    "    eigvec = eigvec_all[:,index_eig]\n",
    "\n",
    "    # Build the diagonal matrix with the filtered eigenvalues or square\n",
    "    # root of the filtered eigenvalues according to the parameter\n",
    "    if ret_sqrt:\n",
    "        diag = torch.diag(torch.sqrt(1/eigval))\n",
    "    else:\n",
    "        diag = torch.diag(1/eigval)\n",
    "#     print(diag.shape, eigvec.shape)    \n",
    "    # Rebuild the square root of the inverse matrix\n",
    "    x_inv = torch.matmul(eigvec, torch.matmul(diag, eigvec.T))\n",
    "\n",
    "    return x_inv\n",
    "\n",
    "\n",
    "def _prep_data(data_t, data_tau):\n",
    "    '''Utility function that transorms the input data from a tensorflow - \n",
    "    viable format to a structure used by the following functions in the\n",
    "    pipeline.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: tensorflow tensor with shape [b, 2*o]\n",
    "        original format of the data\n",
    "    Returns\n",
    "    -------\n",
    "    x: tensorflow tensor with shape [o, b]\n",
    "        transposed, mean-free data corresponding to the left, lag-free lobe\n",
    "        of the network\n",
    "\n",
    "    y: tensorflow tensor with shape [o, b]\n",
    "        transposed, mean-free data corresponding to the right, lagged lobe\n",
    "        of the network\n",
    "\n",
    "    b: tensorflow float32\n",
    "        batch size of the data\n",
    "\n",
    "    o: int\n",
    "        output size of each lobe of the network\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Subtract the mean\n",
    "    x = data_t #- torch.mean(data_t, dim=0, keepdim=True)\n",
    "    y = data_tau# - torch.mean(data_tau, dim=0, keepdim=True)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the classes for the model and the attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(torch.nn.Module):\n",
    "    ''' Attention mask either independent from the time point (mask_const=True) or dependent.\n",
    "    If dependent the attention is estimated via a NN with depth and width given as input, which are \n",
    "    otherwise ignored. \n",
    "    The attention mechanism assumes that distances are used. skip_res is number of residues skiped when estimating\n",
    "    the distance. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, input_size, mask_const, depth=0, width=100 , patchsize=4, sensitivity=1, fac=True,\n",
    "                noise=0.):\n",
    "        super(Mask, self).__init__()\n",
    "        \n",
    "#         self.alpha = torch.Tensor(1, input_size, N, heads).fill_(0)\n",
    "\n",
    "        skip_res = 3\n",
    "        self.noise = noise\n",
    "        self.n_residues = int(-1/2 + np.sqrt(1/4+input_size*2) + skip_res)\n",
    "        \n",
    "        self.bs_per_res = [[] for _ in range(patchsize)]\n",
    "        \n",
    "        self.residues_1 = []\n",
    "        self.residues_2 = []\n",
    "        self.patchsize = patchsize\n",
    "        self.number_weights = self.n_residues + (patchsize-1)\n",
    "        # estimate the pairs\n",
    "        for n1 in range(self.n_residues):\n",
    "            for i in range(patchsize):\n",
    "                self.bs_per_res[i].append(n1+i)\n",
    "            \n",
    "        for n1 in range(self.n_residues-skip_res):\n",
    "            for n2 in range(n1+skip_res, self.n_residues):\n",
    "                self.residues_1.append(n1)\n",
    "                self.residues_2.append(n2)\n",
    "                        \n",
    "                        \n",
    "        self.mask_const = mask_const\n",
    "        \n",
    "        if mask_const:        \n",
    "            self.alpha = torch.randn((1, self.number_weights)) * 0.5\n",
    "            self.weight = torch.nn.Parameter(data=self.alpha, requires_grad=True)\n",
    "        else:\n",
    "            nodes = [input_size]\n",
    "            for i in range(depth):\n",
    "                nodes.append(width)\n",
    "            \n",
    "            \n",
    "            self.hfc = [nn.Linear(nodes[i], nodes[i+1]) for i in range(len(nodes)-1)]\n",
    "            self.softmax = nn.Linear(nodes[-1], self.number_weights, bias=True)\n",
    "            self.layers = nn.ModuleList(self.hfc)\n",
    "            \n",
    "        self.sensitivity = sensitivity\n",
    "        if fac:\n",
    "            self.fac = self.number_weights\n",
    "        else:\n",
    "            self.fac = 1.\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # weights for each residue\n",
    "        if self.mask_const:\n",
    "\n",
    "            weights_for_res = []\n",
    "            for i in range(self.patchsize): # get all weights b for each residue\n",
    "                weights_for_res.append(self.weight[None,:,self.bs_per_res[i]])\n",
    "                \n",
    "            weights_for_res = torch.prod(torch.cat(weights_for_res, dim=0), dim=0) # take the product of the b factors\n",
    "            weights_for_res = F.softmax(weights_for_res, dim=1) # take the softmax over the residues\n",
    "            weights_for_res = torch.sum(weights_for_res, dim=2)  # take the mean over the heads\n",
    "        else:\n",
    "            y = x\n",
    "            for layer in self.hfc:\n",
    "                y = F.elu(layer(y))\n",
    "            y = self.softmax(y) \n",
    "#             y = torch.reshape(y, (-1, self.number_weights))\n",
    "            # make the factors all positive\n",
    "            y = F.softmax(y, dim=1)*self.fac\n",
    "#             y = F.elu(y)+1\n",
    "            weights_for_res = []\n",
    "            for i in range(self.patchsize): # get all weights b for each residue\n",
    "                weights_for_res.append(y[None,:,self.bs_per_res[i]])\n",
    "                \n",
    "            weights_for_res = torch.prod(torch.cat(weights_for_res, dim=0), dim=0) # take the product of the b factors\n",
    "            weights_for_res = F.softmax(weights_for_res, dim=1) # take the softmax over the residues\n",
    "            # take the mean over the heads\n",
    "            \n",
    "        \n",
    "            \n",
    "#         print(torch.cat(weight_all_1, dim=0).shape)\n",
    "        weight_1 = weights_for_res[:,self.residues_1]\n",
    "        weight_2 = weights_for_res[:,self.residues_2]\n",
    "#         print(weight_1.shape)\n",
    "        alpha = weight_1 * weight_2 * self.n_residues**2\n",
    "        masked_x = x * alpha\n",
    "        \n",
    "        if self.noise > 0.:\n",
    "            max_attention_value = torch.max(alpha, dim=1, keepdim=True)[0].detach()\n",
    "#             shape = (x.shape[0], alpha.shape[1], alpha.shape[2])\n",
    "            shape = alpha.shape\n",
    "            random_numbers = torch.randn(shape, device=device) * self.noise\n",
    "            masked_x += (1 - alpha/max_attention_value) * random_numbers\n",
    "        \n",
    "        return masked_x\n",
    "    \n",
    "    def get_softmax(self, x=None):\n",
    "        if self.mask_const:\n",
    "            weights_for_res = []\n",
    "            for i in range(self.patchsize): # get all weights b for each residue\n",
    "                weights_for_res.append(self.weight[None,:,self.bs_per_res[i]])\n",
    "                \n",
    "            weights_for_res = torch.prod(torch.cat(weights_for_res, dim=0), dim=0) # take the product of the b factors\n",
    "            weights_for_res = F.softmax(weights_for_res, dim=1) # take the softmax over the residues\n",
    "        else:\n",
    "            y = x\n",
    "            for layer in self.hfc:\n",
    "                y = layer(y)\n",
    "            y = self.softmax(y)\n",
    "            y = F.softmax(y, dim=1)*self.fac\n",
    "#             y = F.elu(y)+1\n",
    "            weights_for_res = []\n",
    "            for i in range(self.patchsize): # get all weights b for each residue\n",
    "                weights_for_res.append(y[None,:,self.bs_per_res[i]])\n",
    "                \n",
    "            weights_for_res = torch.prod(torch.cat(weights_for_res, dim=0), dim=0) # take the product of the b factors\n",
    "            weights_for_res = F.softmax(weights_for_res, dim=1) # take the softmax over the residues\n",
    "            \n",
    "#         weight_sf = torch.sum(F.softmax(self.weight*self.sensitivity, dim=self.dim), dim=3)\n",
    "        \n",
    "        return weights_for_res\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        if self.mask_const:\n",
    "            alpha = torch.Tensor(weights)\n",
    "\n",
    "            self.weight = torch.nn.Parameter(data=alpha, requires_grad=True)\n",
    "        else:\n",
    "            print('not implemented yet. You need to define all layers in the mask')\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "class Coarse_grain(torch.nn.Module):\n",
    "    ''' Attention mask either independent from the time point (mask_const=True) or dependent.\n",
    "    If dependent the attention is estimated via a NN with depth and width given as input, which are \n",
    "    otherwise ignored. \n",
    "    The attention mechanism assumes that distances are used. skip_res is number of residues skiped when estimating\n",
    "    the distance. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, sen=1):\n",
    "        super(Coarse_grain, self).__init__()\n",
    "\n",
    "        self.N = input_dim\n",
    "        self.M = output_dim\n",
    "        self.sen = sen\n",
    "        \n",
    "        self.alpha = torch.randn((self.N, self.M)) * 0.5\n",
    "        self.weight = torch.nn.Parameter(data=self.alpha, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        kernel = F.softmax(self.sen * self.weight, dim=1)\n",
    "        \n",
    "        ret = x @ kernel\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_softmax(self):\n",
    "        \n",
    "        return F.softmax(self.sen * self.weight, dim=1)\n",
    "    \n",
    "    def get_cg_uS(self, chi_n, chi_tau_n, u_n, S_n, u_t_n, renorm):\n",
    "        \n",
    "        batchsize = chi_n.shape[0]\n",
    "        M = F.softmax(self.sen * self.weight, dim=1)\n",
    "        \n",
    "        chi_t_m = chi_n @ M\n",
    "        chi_tau_m = chi_tau_n @ M\n",
    "        \n",
    "        # estimate the pseudo inverse of M\n",
    "        U, S_vec, V = torch.svd(M)\n",
    "        s_nonzero = S_vec > 0\n",
    "        s_zero = S_vec <= 0\n",
    "        S_star = torch.cat((1/S_vec[s_nonzero], S_vec[s_zero]))\n",
    "        U_star = torch.cat((U[:,s_nonzero], U[:,s_zero]), dim=1)\n",
    "        V_star = torch.cat((V[:,s_nonzero], V[:,s_zero]), dim=1)\n",
    "        G = V_star @ torch.diag(S_star) @ U_star.T\n",
    "        \n",
    "        # estimate the new u and S\n",
    "        u_m = (G @ u_n.T).T\n",
    "        # renormalize\n",
    "        chi_mean = torch.mean(chi_tau_m, dim=0, keepdim=True)\n",
    "        u_m = u_m / torch.sum(chi_mean * u_m, dim=1, keepdim=True)\n",
    "        \n",
    "        u_t_m = (G @ u_t_n.T).T\n",
    "        chi_mean_t = torch.mean(chi_t_m, dim=0, keepdim=True)\n",
    "        u_t_m = u_t_m / torch.sum(chi_mean_t * u_t_m, dim=1, keepdim=True)\n",
    "        \n",
    "        W1 = G @ S_n @ G.T\n",
    "        #renormalize\n",
    "        batchsize = chi_n.shape[0]\n",
    "        corr_tau = 1./batchsize * torch.matmul(chi_tau_m.T, chi_tau_m)\n",
    "        v = torch.matmul(corr_tau, u_m.T)\n",
    "        norm = W1 @ v\n",
    "        \n",
    "        \n",
    "        w2 = (1 - torch.squeeze(norm)) / torch.squeeze(v)\n",
    "        S_temp = W1 + torch.diag(w2)\n",
    "        if renorm:\n",
    "            \n",
    "            if (S_temp<0).sum()>0: # check if actually non-negativity is violated\n",
    "                \n",
    "                # make sure that the largest value of norm is < 1\n",
    "                quasi_inf_norm = lambda x: torch.sum((x**20))**(1./20)\n",
    "    #             print(norm, quasi_inf_norm(norm))\n",
    "                W1 = W1 / quasi_inf_norm(norm)\n",
    "                norm = W1 @ v\n",
    "                \n",
    "                w2 = (1 - torch.squeeze(norm)) / torch.squeeze(v)\n",
    "                S_temp = W1 + torch.diag(w2)\n",
    "                \n",
    "                \n",
    "        S_m = S_temp\n",
    "        \n",
    "        \n",
    "        # estimate the VAMP-E matrix and other helpful instances\n",
    "        mu = 1./batchsize * torch.matmul(chi_tau_m, u_m.T)\n",
    "        Sigma =  torch.matmul((chi_tau_m * mu).T, chi_tau_m)\n",
    "        \n",
    "        \n",
    "        mu_t = 1./batchsize * torch.matmul(chi_t_m, u_t_m.T)\n",
    "        Sigma_t =  torch.matmul((chi_t_m * mu_t).T, chi_t_m)\n",
    "\n",
    "        gamma = chi_tau_m * (torch.matmul(chi_tau_m, u_m.T))\n",
    "\n",
    "        C_00 = 1./batchsize * torch.matmul(chi_t_m.T, chi_t_m)\n",
    "        C_11 = 1./batchsize * torch.matmul(gamma.T, gamma)\n",
    "        C_01 = 1./batchsize * torch.matmul(chi_t_m.T, gamma)\n",
    "        \n",
    "        \n",
    "        K = S_m @ Sigma\n",
    "\n",
    "        # VAMP-E matrix for the computation of the loss\n",
    "        VampE_matrix = S_m.T @ C_00 @ S_m @ C_11 - 2*S_m.T @ C_01\n",
    "        \n",
    "        ret = [\n",
    "            chi_t_m, \n",
    "            chi_tau_m, \n",
    "            u_m,\n",
    "            u_t_m,\n",
    "            S_m,\n",
    "            mu_t,\n",
    "            Sigma_t,\n",
    "            K,\n",
    "            VampE_matrix\n",
    "        ]\n",
    "        return ret\n",
    "            \n",
    "    def reset_params(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            self.weight.copy_(torch.randn((self.N, self.M)) * 0.5) \n",
    "        \n",
    "#         alpha = torch.randn((self.N, self.M)) * 0.5\n",
    "#         self.weight = torch.nn.Parameter(data=alpha, requires_grad=True)\n",
    "        \n",
    "class U_layer(torch.nn.Module):\n",
    "    ''' Attention mask either independent from the time point (mask_const=True) or dependent.\n",
    "    If dependent the attention is estimated via a NN with depth and width given as input, which are \n",
    "    otherwise ignored. \n",
    "    The attention mechanism assumes that distances are used. skip_res is number of residues skiped when estimating\n",
    "    the distance. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, output_dim, activation):\n",
    "        super(U_layer, self).__init__()\n",
    "\n",
    "        self.M = output_dim\n",
    "        \n",
    "        self.alpha = torch.Tensor(1, self.M).fill_(1/self.M)\n",
    "        self.u_kernel = torch.nn.Parameter(data=self.alpha, requires_grad=True)\n",
    "        self.acti = activation\n",
    "        \n",
    "    def forward(self, chi_t, chi_tau):\n",
    "        \n",
    "        # we need batchsize to stack the outputs later so that it fullfills keras' requirements\n",
    "        batchsize = chi_t.shape[0]\n",
    "\n",
    "        # note: corr_tau is the correlation matrix of the time-shifted data\n",
    "        # presented in the paper at page 6, \"Normalization of transition density\"\n",
    "        corr_tau = 1./batchsize * torch.matmul(chi_tau.T, chi_tau)\n",
    "        chi_mean = torch.mean(chi_tau, dim=0, keepdim=True)\n",
    "\n",
    "        kernel_u = self.acti(self.u_kernel)\n",
    "\n",
    "        # u is the normalized and transformed kernel of this layer\n",
    "        u = kernel_u / torch.sum(chi_mean * kernel_u, dim=1, keepdim=True)\n",
    "\n",
    "        v = torch.matmul(corr_tau, u.T)\n",
    "\n",
    "        mu = 1./batchsize * torch.matmul(chi_tau, u.T)\n",
    "        \n",
    "        Sigma =  torch.matmul((chi_tau * mu).T, chi_tau)\n",
    "        \n",
    "        chi_mean_t = torch.mean(chi_t, dim=0, keepdim=True)\n",
    "        u_t = kernel_u / torch.sum(chi_mean_t * kernel_u, dim=1, keepdim=True)\n",
    "        mu_t = 1./batchsize * torch.matmul(chi_t, u_t.T)\n",
    "        Sigma_t =  torch.matmul((chi_t * mu_t).T, chi_t)\n",
    "\n",
    "        gamma = chi_tau * (torch.matmul(chi_tau, u.T))\n",
    "\n",
    "        C_00 = 1./batchsize * torch.matmul(chi_t.T, chi_t)\n",
    "        C_11 = 1./batchsize * torch.matmul(gamma.T, gamma)\n",
    "        C_01 = 1./batchsize * torch.matmul(chi_t.T, gamma)\n",
    "\n",
    "\n",
    "        ret = [\n",
    "            u,\n",
    "            u_t,\n",
    "            v,\n",
    "            C_00,\n",
    "            C_11,\n",
    "            C_01,\n",
    "            Sigma,\n",
    "            mu_t,\n",
    "            Sigma_t\n",
    "        ]\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    \n",
    "class S_layer(torch.nn.Module):\n",
    "    ''' Attention mask either independent from the time point (mask_const=True) or dependent.\n",
    "    If dependent the attention is estimated via a NN with depth and width given as input, which are \n",
    "    otherwise ignored. \n",
    "    The attention mechanism assumes that distances are used. skip_res is number of residues skiped when estimating\n",
    "    the distance. \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, output_dim, activation, renorm=True):\n",
    "        super(S_layer, self).__init__()\n",
    "\n",
    "        self.M = output_dim\n",
    "        \n",
    "        self.alpha = torch.Tensor(self.M, self.M).fill_(0.1)\n",
    "        self.S_kernel = torch.nn.Parameter(data=self.alpha, requires_grad=True)\n",
    "        self.acti = activation\n",
    "        self.renorm = renorm\n",
    "        \n",
    "    def forward(self, v, C_00, C_11, C_01, Sigma):\n",
    "        \n",
    "        # we need batchsize to stack the outputs later so that it fullfills keras' requirements\n",
    "            \n",
    "        batchsize = v.shape[0]\n",
    "\n",
    "        # transform the kernel weights\n",
    "        kernel_w = self.acti(self.S_kernel)\n",
    "        \n",
    "        # enforce symmetry\n",
    "        W1 = kernel_w + kernel_w.T\n",
    "\n",
    "        # normalize the weights\n",
    "        norm = W1 @ v\n",
    "        \n",
    "        \n",
    "        w2 = (1 - torch.squeeze(norm)) / torch.squeeze(v)\n",
    "        S_temp = W1 + torch.diag(w2)\n",
    "        if self.renorm:\n",
    "            \n",
    "#             if (S_temp<0).sum()>0: # check if actually non-negativity is violated\n",
    "                \n",
    "            # make sure that the largest value of norm is < 1\n",
    "            quasi_inf_norm = lambda x: torch.sum((x**20))**(1./20)\n",
    "#             print(norm, quasi_inf_norm(norm))\n",
    "            W1 = W1 / quasi_inf_norm(norm)\n",
    "            norm = W1 @ v\n",
    "\n",
    "            w2 = (1 - torch.squeeze(norm)) / torch.squeeze(v)\n",
    "            S_temp = W1 + torch.diag(w2)\n",
    "                \n",
    "                \n",
    "        S = S_temp\n",
    "\n",
    "        # calculate K\n",
    "        K = S @ Sigma\n",
    "\n",
    "        # VAMP-E matrix for the computation of the loss\n",
    "        VampE_matrix = S.T @ C_00 @ S @ C_11 - 2*S.T @ C_01\n",
    "\n",
    "        # stack outputs so that the first dimension is = batchsize, keras requirement\n",
    "        \n",
    "        ret = [VampE_matrix, K, S]\n",
    "        \n",
    "        return ret\n",
    "            \n",
    "class VampNet(nn.Module):\n",
    "    ''' VAMPnet class\n",
    "    TODO:\n",
    "    - revVAMP\n",
    "    - revDMSM\n",
    "    - DMSM\n",
    "    - VAMP\n",
    "    \n",
    "    inputs:\n",
    "    \n",
    "    input_size: (int) size of the input features\n",
    "    output_sizes: (list & int) list of output sizes. If more than one elemtent, expects coarse graining\n",
    "    nodes: (list & int) list of output size of hidden layers\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self, input_size, output_sizes, nodes, train_mean, train_std, \n",
    "                 valid_T=False, reversible=False,\n",
    "                 mask_const=True, mask_depth=0, mask_width=0, patchsize=4, sensitivity=1, fac=True,\n",
    "                 noise=0.,\n",
    "                 softmax_fac=1.):\n",
    "        super(VampNet, self).__init__()\n",
    "        \n",
    "        # which physical constraints are enacted which can need more parameters with different contraints\n",
    "        # Furthermore the best learning practice is different\n",
    "        self.valid_T = valid_T\n",
    "        self.reversible = reversible\n",
    "        if valid_T:\n",
    "            model='DMSM'\n",
    "        else:\n",
    "            model='VAMPnet'\n",
    "        if reversible:\n",
    "            rev='rev'\n",
    "        else:\n",
    "            rev=''\n",
    "        print('The trained model is a '+rev+model) \n",
    "        \n",
    "        if valid_T and not reversible:\n",
    "            self.gamma = True\n",
    "        else:\n",
    "            self.gamma = False\n",
    "        if reversible:\n",
    "            if valid_T:\n",
    "                self.renorm = True\n",
    "                acti_S = torch.nn.Softplus()\n",
    "                acti_u = torch.nn.Softplus()\n",
    "                \n",
    "            else: \n",
    "                self.factor_S = 0.001\n",
    "                self.factor_u = .000001\n",
    "\n",
    "                linear_S = lambda x: self.factor_S * x\n",
    "                linear_u = lambda x: self.factor_u * x\n",
    "                self.renorm = False\n",
    "                acti_S = linear_S\n",
    "                acti_u = linear_u\n",
    "            \n",
    "            \n",
    "            self.u_layers = [U_layer(o, acti_u) for o in output_sizes]\n",
    "            self.S_layers = [S_layer(o, acti_S, self.renorm) for o in output_sizes]\n",
    "            self.u_layers_tensor = nn.ModuleList(self.u_layers)\n",
    "            self.S_layers_tensor = nn.ModuleList(self.S_layers)\n",
    "                \n",
    "# activations for reversible VAMPnets\n",
    "\n",
    "\n",
    "#activations for revDMSM\n",
    "\n",
    "        \n",
    "        \n",
    "        self.N = len(output_sizes)\n",
    "        self.output_sizes = output_sizes\n",
    "        \n",
    "        self.softmax_fac = softmax_fac\n",
    "        self.fac = fac\n",
    "        self.nodes = nodes\n",
    "        self.mask_const = mask_const\n",
    "        self.Mask = Mask(input_size, mask_const, mask_depth, mask_width, patchsize, sensitivity, fac=fac, noise=noise)\n",
    "        \n",
    "        self.hfc = nn.ModuleList([nn.Linear(input_size, nodes[0])])\n",
    "        for i in range(len(nodes)-1):\n",
    "            self.hfc.append(nn.Linear(nodes[i], nodes[i+1]))\n",
    "            \n",
    "        \n",
    "        self.fc_softmax = nn.Linear(nodes[-1], output_sizes[0])\n",
    "        \n",
    "        self.gamma_layer = nn.Linear(nodes[-1], output_sizes[0])\n",
    "            # is just needed once, not for coarse graining, since it convert the same as chi\n",
    "        \n",
    "        self.coarse_grain_layer = [Coarse_grain(output_sizes[n], output_sizes[n+1]).to(device) for n in range(len(output_sizes)-1)]\n",
    "        self.coarse_grain_layer_tensor = nn.ModuleList(self.coarse_grain_layer)\n",
    "        self.train_mean = torch.Tensor(train_mean).to(device)\n",
    "        self.train_std = torch.Tensor(train_std).to(device)\n",
    "    \n",
    "    def forward_before_sm(self, x):\n",
    "        \n",
    "        x = (x-self.train_mean)/self.train_std\n",
    "        shape = x.shape\n",
    "        b = shape[0]\n",
    "        \n",
    "        x = self.Mask(x) # b x input_size\n",
    "\n",
    "        \n",
    "        for layer_list_i in self.hfc:\n",
    "\n",
    "            x = F.elu(layer_list_i(x))\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.forward_before_sm(x)\n",
    "\n",
    "        x_output = F.softmax(self.softmax_fac*self.fc_softmax(x), dim=1)\n",
    "        \n",
    "        return x_output\n",
    "    \n",
    "    def forward_cg(self, x, id):\n",
    "        \n",
    "        x_output = self.coarse_grain_layer[id](x)\n",
    "        \n",
    "        return x_output\n",
    "    \n",
    "    def forward_all(self, x):\n",
    "        outputs = []\n",
    "        \n",
    "        x = self.forward(x)\n",
    "        \n",
    "        outputs.append(x)\n",
    "        \n",
    "        for layer in self.coarse_grain_layer:\n",
    "            x = layer(x)\n",
    "            outputs.append(x)\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def forward_gamma(self, x, whole=True):\n",
    "        if whole:\n",
    "            x = self.forward_before_sm(x)\n",
    "        x_gamma = F.relu(self.gamma_layer(x)) # plus 1 so it is always positive\n",
    "        \n",
    "        return x_gamma\n",
    "    \n",
    "        \n",
    "    def get_attention(self, x=None):\n",
    "        return self.Mask.get_softmax(x=x)\n",
    "    \n",
    "    def set_soft_fac(self, new_value):\n",
    "        self.softmax_fac = torch.Tensor(new_value)\n",
    "        \n",
    "    def set_soft_fac_cg(self, id_cg, new_value):\n",
    "        \n",
    "        self.coarse_grain_layer[id_cg].sen = new_value\n",
    "    \n",
    "    def get_params_vamp(self):\n",
    "        if self.mask_const:\n",
    "            for param in self.Mask.parameters():\n",
    "                yield param\n",
    "        else:\n",
    "            for layer in self.Mask.hfc:\n",
    "                for param in layer.parameters():\n",
    "                    yield param\n",
    "            for param in self.Mask.softmax.parameters():\n",
    "                yield param\n",
    "                    \n",
    "        for layer in self.hfc:\n",
    "            \n",
    "            for param in layer.parameters():\n",
    "                yield param\n",
    "        \n",
    "        for param in self.fc_softmax.parameters():\n",
    "            yield param\n",
    "    \n",
    "    def get_params_DMSM(self, all=True):\n",
    "        if all:\n",
    "            if self.mask_const:\n",
    "                for param in self.Mask.parameters():\n",
    "                    yield param\n",
    "            else:\n",
    "                for layer in self.Mask.hfc:\n",
    "                    for param in layer.parameters():\n",
    "                        yield param\n",
    "                for param in self.Mask.softmax.parameters():\n",
    "                    yield param\n",
    "\n",
    "            for layer in self.hfc:\n",
    "\n",
    "                for param in layer.parameters():\n",
    "                    yield param\n",
    "\n",
    "            for param in self.fc_softmax.parameters():\n",
    "                yield param\n",
    "        for param in self.gamma_layer.parameters():\n",
    "            yield param\n",
    "\n",
    "    def get_params_rev(self, all=True, u_flag=True, S_flag=True):\n",
    "        if all:\n",
    "            if self.mask_const:\n",
    "                for param in self.Mask.parameters():\n",
    "                    yield param\n",
    "            else:\n",
    "                for layer in self.Mask.hfc:\n",
    "                    for param in layer.parameters():\n",
    "                        yield param\n",
    "                for param in self.Mask.softmax.parameters():\n",
    "                    yield param\n",
    "\n",
    "            for layer in self.hfc:\n",
    "\n",
    "                for param in layer.parameters():\n",
    "                    yield param\n",
    "\n",
    "            for param in self.fc_softmax.parameters():\n",
    "                yield param\n",
    "        if u_flag:\n",
    "            for param in self.u_layers[0].parameters():\n",
    "                yield param\n",
    "        if S_flag:\n",
    "            for param in self.S_layers[0].parameters():\n",
    "                yield param\n",
    "    \n",
    "    def get_params_all(self, u_flag=False, S_flag=False):\n",
    "        if self.mask_const:\n",
    "            for param in self.Mask.parameters():\n",
    "                yield param\n",
    "        else:\n",
    "            for layer in self.Mask.hfc:\n",
    "                for param in layer.parameters():\n",
    "                    yield param\n",
    "            for param in self.Mask.softmax.parameters():\n",
    "                yield param\n",
    "                    \n",
    "        for layer in self.hfc:\n",
    "            \n",
    "            for param in layer.parameters():\n",
    "                yield param\n",
    "        \n",
    "        for param in self.fc_softmax.parameters():\n",
    "            yield param\n",
    "        for layer in self.coarse_grain_layer:\n",
    "            for param in layer.parameters():\n",
    "                yield param\n",
    "                \n",
    "        if u_flag:\n",
    "            for param in self.u_layers[0].parameters():\n",
    "                yield param\n",
    "        if S_flag:\n",
    "            for param in self.S_layers[0].parameters():\n",
    "                yield param\n",
    "        \n",
    "    \n",
    "    def get_params_wo_mask(self):\n",
    "        for layer in self.hfc:\n",
    "            for param in layer.parameters():\n",
    "                yield param\n",
    "       \n",
    "        for param in self.fc_softmax.parameters():\n",
    "            yield param\n",
    "    \n",
    "    def get_params_softmax(self):\n",
    "        \n",
    "        for param in self.fc_softmax.parameters():\n",
    "            yield param\n",
    "            \n",
    "    \n",
    "    def get_params_mask(self):\n",
    "        if self.mask_const:\n",
    "            for param in self.Mask.parameters():\n",
    "                yield param\n",
    "        else:\n",
    "            for layer in self.Mask.hfc:\n",
    "                for param in layer.parameters():\n",
    "                    yield param\n",
    "            for param in self.Mask.softmax.parameters():\n",
    "                yield param\n",
    "            \n",
    "    def get_params_cg(self, index=[]):\n",
    "        for id in index:\n",
    "            for param in self.coarse_grain_layer[id].parameters():\n",
    "                yield param\n",
    "    \n",
    "    def get_params_cg_rev(self, index=[], all=True):\n",
    "        \n",
    "        for id in index:\n",
    "            if all:\n",
    "                for param in self.coarse_grain_layer[id].parameters():\n",
    "                    yield param\n",
    "            for param in self.u_layers[id+1].parameters():\n",
    "                yield param\n",
    "            for param in self.S_layers[id+1].parameters():\n",
    "                yield param\n",
    "                \n",
    "                \n",
    "    def set_rev_var(self, layer_id=0, S=True):\n",
    "        \n",
    "        chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "        chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "    \n",
    "        for i in range(layer_id):\n",
    "            chi_t = self.forward_cg(chi_t, i)\n",
    "            chi_tau = self.forward_cg(chi_tau, i)\n",
    "            \n",
    "        Data_chi_X = chi_t.detach().to('cpu').numpy()\n",
    "        Data_chi_Y = chi_tau.detach().to('cpu').numpy()\n",
    "        fullbatch = Data_chi_X.shape[0]\n",
    "\n",
    "\n",
    "        c_0 = 1/fullbatch * Data_chi_X.T @ Data_chi_X\n",
    "        c_tau = 1/fullbatch * Data_chi_X.T @ Data_chi_Y\n",
    "        c_1 = 1/fullbatch * Data_chi_Y.T @ Data_chi_Y\n",
    "\n",
    "        eigv_all, eigvec_all = np.linalg.eigh(c_0)\n",
    "        print(eigv_all)\n",
    "        include = eigv_all > epsilon\n",
    "        eigv = eigv_all[include]\n",
    "        eigvec = eigvec_all[:,include]\n",
    "        c0_inv = eigvec @ np.diag(1/eigv) @ np.transpose(eigvec)\n",
    "\n",
    "        K_vamp = c0_inv @ c_tau\n",
    "\n",
    "        # estimate pi, the stationary distribution vector\n",
    "        eigv, eigvec = np.linalg.eig(K_vamp.T)\n",
    "        ind_pi = np.argmin((eigv-1)**2)\n",
    "\n",
    "        pi_vec = np.real(eigvec[:,ind_pi])\n",
    "        pi = pi_vec / np.sum(pi_vec, keepdims=True)\n",
    "        print('pi', pi)\n",
    "        # reverse the consruction of u \n",
    "        u_optimal = c0_inv @ pi\n",
    "        print('u optimal', u_optimal)\n",
    "        if self.valid_T:\n",
    "            u_kernel = np.log(np.exp(np.abs(u_optimal))-1)\n",
    "        else:\n",
    "            u_kernel = u_optimal / self.factor_u\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in self.u_layers[layer_id].parameters():\n",
    "            \n",
    "                param.copy_(torch.Tensor(u_kernel[None,:]))  \n",
    "            \n",
    "        if S:\n",
    "            \n",
    "            _, _, _, _, _, _, Sigma_input, _, _ = self.u_layers[layer_id](chi_t, chi_tau)\n",
    "            Sigma = Sigma_input.detach().to('cpu').numpy()\n",
    "            \n",
    "            eigv_all, eigvec_all = np.linalg.eigh(Sigma)\n",
    "            include = eigv_all > epsilon\n",
    "            eigv = eigv_all[include]\n",
    "            eigvec = eigvec_all[:,include]\n",
    "            sigma_inv = eigvec @ np.diag(1/eigv) @ np.transpose(eigvec)\n",
    "\n",
    "            # reverse the construction of S\n",
    "            S_nonrev = K_vamp @ sigma_inv\n",
    "            S_rev_add = 1/2 * (S_nonrev + S_nonrev.T)\n",
    "            if self.valid_T:\n",
    "                kernel_S = S_rev_add / 2.\n",
    "                kernel_S = np.log(np.exp(np.abs(kernel_S))-1)\n",
    "            else:\n",
    "                kernel_S = S_rev_add / 2. / self.factor_S\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for param in self.S_layers[layer_id].parameters():\n",
    "\n",
    "                    param.copy_(torch.Tensor(kernel_S)) \n",
    "            \n",
    "    def get_weights(self):\n",
    "        \n",
    "        weights_dict = {}\n",
    "        weights_dict['hfc'] = []\n",
    "        weights_dict['sm'] = []\n",
    "\n",
    "        for layer in  self.hfc:\n",
    "            for param in layer.parameters():\n",
    "                weights_dict['hfc'].append(param.detach().to('cpu').numpy().copy())\n",
    "\n",
    "        for param in self.fc_softmax.parameters():\n",
    "            weights_dict['sm'].append(param.detach().to('cpu').numpy().copy())\n",
    "\n",
    "        if self.reversible:\n",
    "            weights_dict['S'] = [param.detach().to('cpu').numpy().copy() for param in self.S_layers[0].parameters()]\n",
    "            weights_dict['u'] = [param.detach().to('cpu').numpy().copy() for param in self.u_layers[0].parameters()]\n",
    "\n",
    "        if self.mask_const:\n",
    "            weights_dict['Mask'] = [param.detach().to('cpu').numpy().copy() for param in self.Mask.parameters()]\n",
    "        else:\n",
    "            weights_dict['Mask_hf'] = []\n",
    "            weights_dict['Mask_sm'] = []\n",
    "\n",
    "            for layer in self.Mask.hfc:\n",
    "                for param in layer.parameters():\n",
    "                    weights_dict['Mask_hf'].append(param.detach().to('cpu').numpy().copy())\n",
    "            for param in self.Mask.softmax.parameters():\n",
    "                weights_dict['Mask_sm'].append(param.detach().to('cpu').numpy().copy())\n",
    "\n",
    "        weights_dict['cg'] = []\n",
    "        weights_dict['S_cg'] = []\n",
    "        weights_dict['u_cg'] = []\n",
    "        for i, layer in enumerate(self.coarse_grain_layer):\n",
    "            for param in layer.parameters():\n",
    "                weights_dict['cg'].append(param.detach().to('cpu').numpy().copy())\n",
    "            weights_dict['S_cg'].append([param.detach().to('cpu').numpy().copy() for param in self.S_layers[i+1].parameters()][0])\n",
    "            weights_dict['u_cg'].append([param.detach().to('cpu').numpy().copy() for param in self.u_layers[i+1].parameters()][0])\n",
    "        weights_dict['train_mean'] = self.train_mean.to('cpu').numpy()\n",
    "        weights_dict['train_std'] = self.train_std.to('cpu').numpy()\n",
    "        \n",
    "        return weights_dict\n",
    "    \n",
    "    def set_weights(self, weights_dict):\n",
    "        self.train_mean = torch.Tensor(weights_dict['train_mean']).to(device)\n",
    "        self.train_std = torch.Tensor(weights_dict['train_std']).to(device)\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            for layer in self.hfc:\n",
    "                for param in layer.parameters():    \n",
    "                    param.copy_(torch.Tensor(weights_dict['hfc'][i])) \n",
    "                    i+=1\n",
    "            i = 0\n",
    "            for param in self.fc_softmax.parameters():\n",
    "                param.copy_(torch.Tensor(weights_dict['sm'][i]))\n",
    "                i+=1\n",
    "\n",
    "            if self.reversible:\n",
    "                i=0\n",
    "                for param in self.S_layers[0].parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['S'][i]))\n",
    "                    i+=1\n",
    "                i=0\n",
    "                for param in self.u_layers[0].parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['u'][i]))\n",
    "                    i+=1\n",
    "\n",
    "            if self.mask_const:\n",
    "                i=0\n",
    "                for param in self.Mask.parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['Mask'][i]))\n",
    "                    i+=1\n",
    "            else:\n",
    "                i=0\n",
    "                for layer in self.Mask.hfc:\n",
    "                    for param in layer.parameters():\n",
    "                        param.copy_(torch.Tensor(weights_dict['Mask_hf'][i]))\n",
    "                        i+=1\n",
    "                i=0\n",
    "                for param in self.Mask.softmax.parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['Mask_sm'][i]))\n",
    "                    i+=1\n",
    "            i=0\n",
    "            for layer in self.coarse_grain_layer:\n",
    "\n",
    "                for param in layer.parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['cg'][i]))\n",
    "                for param in self.S_layers[i+1].parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['S_cg'][i]))\n",
    "                for param in self.u_layers[i+1].parameters():\n",
    "                    param.copy_(torch.Tensor(weights_dict['u_cg'][i]))\n",
    "                i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the VAMP-2 and VAMP-E score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAMP_score(chi_t, chi_tau, corr=False):\n",
    "    '''Calculates the VAMP-2 score with respect to the network lobes while \n",
    "    symmetrizing the correlation matrices. Can be used as a loss function\n",
    "    for keras models.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chi_t: tensorflow tensor.\n",
    "        parameter not needed for the calculation, added to comply with Keras\n",
    "        rules for loss fuctions format.\n",
    "\n",
    "    chi_tau: tensorflow tensor with shape [batch_size, 2 * output_size]\n",
    "        output of the two lobes of the network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss_score: tensorflow tensor with shape [1].\n",
    "    '''\n",
    "    shape = chi_t.shape\n",
    "    \n",
    "        \n",
    "    batch_size = shape[0]\n",
    "\n",
    "#     weights = weights * (1-torch.sum(chi_t*chi_tau, dim=1, keepdim=True))\n",
    "\n",
    "\n",
    "    x, y = _prep_data(chi_t, chi_tau) \n",
    "\n",
    "    # Calculate the covariance matrices\n",
    "    cov_00 = 1/(batch_size - 1) * torch.matmul(x.T, x) \n",
    "    cov_11 = 1/(batch_size - 1) * torch.matmul(y.T, y)\n",
    "    cov_01 = 1/(batch_size - 1) * torch.matmul(x.T, y)\n",
    "    \n",
    "#         print(cov_00)\n",
    "    # Calculate the inverse of the self-covariance matrices\n",
    "    cov_00_inv = _inv(cov_00, ret_sqrt = True)\n",
    "    cov_11_inv = _inv(cov_11, ret_sqrt = True)\n",
    "    \n",
    "\n",
    "    # Estimate Vamp-matrix\n",
    "    vamp_matrix = torch.matmul(cov_00_inv, torch.matmul(cov_01, cov_11_inv))\n",
    "    \n",
    "    \n",
    "    vamp_score = torch.norm(vamp_matrix)\n",
    "#     u, sing_values, v = torch.svd(vamp_matrix, compute_uv=True)\n",
    "       \n",
    "#     ind = sing_values<=1.\n",
    "#     score = sing_values[ind].sum().unsqueeze(0)\n",
    "    score = (vamp_score**2).unsqueeze(0)\n",
    "    if corr:\n",
    "        return score, torch.trace(cov_00)\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vampe_loss(chi, gamma):\n",
    "    b = chi.shape[0]\n",
    "    \n",
    "    c00 = 1/b*(torch.matmul(chi.T, chi))\n",
    "    c11 = 1/b*(torch.matmul(gamma.T, gamma))\n",
    "    c01 = 1/b*(torch.matmul(chi.T, gamma))\n",
    "\n",
    "    gamma_dia_inv = torch.diag(1/(torch.mean(gamma, dim=0)))  # add something so no devide by zero\n",
    "\n",
    "    first_term = c00 @ gamma_dia_inv @ c11 @ gamma_dia_inv\n",
    "    second_term = 2 * (c01 @ gamma_dia_inv)\n",
    "    vampe_arg = first_term - second_term\n",
    "    vampe = torch.trace(vampe_arg)\n",
    "    \n",
    "    return -vampe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the losses needed for the reversible model if trained for different instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vampe_loss_rev(chi_t, chi_tau, layer_id=0, return_mu=False, return_mu_K_Sigma=False):\n",
    "    \n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[layer_id](chi_t, chi_tau)\n",
    "    matrix, K, _ = Full_net.S_layers[layer_id](v, C_00, C_11, C_01, Sigma)\n",
    "    vampe = torch.trace(matrix)\n",
    "    \n",
    "    if return_mu:\n",
    "        return -vampe, mu_t\n",
    "    \n",
    "    elif return_mu_K_Sigma:\n",
    "        return -vampe, mu_t, K, Sigma_t\n",
    "    \n",
    "    else:\n",
    "        return -vampe\n",
    "    \n",
    "def vampe_loss_rev_only_S(v, C_00, C_11, C_01, Sigma, layer_id=0):\n",
    "    \n",
    "    matrix, K, _ = Full_net.S_layers[layer_id](v, C_00, C_11, C_01, Sigma)\n",
    "#     print(K)\n",
    "    vampe = torch.trace(matrix)\n",
    "    \n",
    "    return -vampe\n",
    "\n",
    "def vampe_loss_rev_cg(chi_t, chi_tau, u, S, u_t, layer_id=0, return_mu=False, return_mu_K_Sigma=False, renorm=True):\n",
    "    \n",
    "    \n",
    "    # only this line should be the part of it\n",
    "    chi_t_m, chi_tau_m, u_m, u_t_m, S_m, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[layer_id].get_cg_uS(\n",
    "                                                                        chi_t, chi_tau, u, S, u_t, renorm)\n",
    "    \n",
    "    vampe = torch.trace(VampE_matrix)\n",
    "    \n",
    "    if return_mu:\n",
    "        return -vampe, mu_t\n",
    "    \n",
    "    elif return_mu_K_Sigma:\n",
    "        return -vampe, mu_t, K, Sigma_t\n",
    "    \n",
    "    else:\n",
    "        return -vampe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coarse graining matrix\n",
    "def plot_cg(id):\n",
    "    attention = Full_net.coarse_grain_layer[id].get_softmax()\n",
    "    attention_np = attention.detach().to('cpu').numpy()\n",
    "    plt.imshow(attention_np)\n",
    "    plt.xlabel('From State', fontsize=18)\n",
    "    plt.ylabel('To State', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform a trajectory which might not fit into memory at once, predict batchwise\n",
    "def pred_batchwise(traj, batchsize=10000):\n",
    "    \n",
    "    data_size = traj.shape[0]\n",
    "    batches = data_size//batchsize\n",
    "    pred_all = []\n",
    "    for i in range(batches):\n",
    "        s = batchsize*i\n",
    "        e = s+batchsize\n",
    "        pred_temp = Full_net.forward(torch.Tensor(traj[s:e]).to(device)).detach().to('cpu').numpy()\n",
    "        pred_all.append(pred_temp)\n",
    "    if batches==0:\n",
    "        pred_all.append(Full_net.forward(torch.Tensor(traj).to(device)).detach().to('cpu').numpy())\n",
    "    else:\n",
    "        pred_all.append(Full_net.forward(torch.Tensor(traj[e:]).to(device)).detach().to('cpu').numpy())\n",
    "    \n",
    "    return np.concatenate(pred_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the mask\n",
    "n_residues = int(-1/2 + np.sqrt(1/4+input_size*2) + 3)\n",
    "def plot_mask(return_values=False, skip=5, vmax=1, top=10):\n",
    "    n_residues = int(-1/2 + np.sqrt(1/4+input_size*2) + 3)\n",
    "    if Full_net.mask_const:\n",
    "        attention = Full_net.get_attention()\n",
    "        attention_np = attention.detach().to('cpu').numpy()\n",
    "        att_atom = np.reshape(attention_np, (n_residues,1))\n",
    "        plt.imshow(att_atom, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('System', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(1),['{}'.format(i) for i in range(1)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        if return_values:\n",
    "            return att_atom\n",
    "        \n",
    "    else:\n",
    "        pred_temp = pred_batchwise(traj_whole_new[0], batchsize=10000)\n",
    "        arg_sort = np.argsort(pred_temp, axis=0)\n",
    "        top_x_state = arg_sort[-top:]\n",
    "        states = pred_temp.shape[1]\n",
    "        att_atom = []\n",
    "        for state in range(states):\n",
    "            frames = top_x_state[:,state]\n",
    "            attention = Full_net.get_attention(torch.Tensor(traj_whole_new[0][frames]).to(device))\n",
    "            attention_np = attention.detach().to('cpu').numpy()\n",
    "            att_atom.append(np.mean(attention_np, axis=0, keepdims=True))\n",
    "        att_atom = np.concatenate(att_atom)\n",
    "        \n",
    "        plt.imshow(att_atom.T, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('State', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(states),['{}'.format(i) for i in range(states)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        if return_values:\n",
    "            return att_atom\n",
    "        \n",
    "# plot_mask(vmax=2, top=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loops for the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_VAMPnet(runs, opt_list, weight_corr=1., plot_mask_every=10, verbose=True, plot_training=True, corr=False, best_weights_flag=False):\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_corr = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "    epoch_loss_valid_corr = np.zeros(runs)\n",
    "#     sen = np.linspace(.1,2,10)\n",
    "    noise = Full_net.Mask.noise\n",
    "    if best_weights_flag:\n",
    "        best_score=0.\n",
    "        best_weights = Full_net.get_weights()\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "#         Full_net.\n",
    "#         opt = optimizer_vamp\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "#         Full_net.set_soft_fac(sen[[epoch]])\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "        running_epoch_corr = []\n",
    "        for i, data_batch in enumerate(train_l, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_t, inputs_tau = data_batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            for opt in opt_list:\n",
    "                opt.zero_grad()\n",
    "\n",
    "            # estimate weights\n",
    "            chi_t = Full_net(inputs_t[0].to(device))\n",
    "            chi_tau = Full_net(inputs_tau[0].to(device))\n",
    "\n",
    "            score_list = VAMP_score(chi_t, chi_tau, corr=corr)\n",
    "\n",
    "            if corr:\n",
    "                score_curr = score_list[0]\n",
    "                score_corr = score_list[1]\n",
    "                loss = -score_curr - score_corr * weight_corr\n",
    "                running_epoch_corr.append(score_corr.item())\n",
    "            else:\n",
    "                score_curr = score_list\n",
    "                loss = - score_curr\n",
    "\n",
    "            loss.backward()\n",
    "            for opt in opt_list:\n",
    "                opt.step()\n",
    "\n",
    "            running_epoch_loss.append(score_curr.item())\n",
    "\n",
    "        # validation\n",
    "        Full_net.Mask.noise = 0.\n",
    "        chi_t_vali = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "        chi_tau_vali = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "        Full_net.Mask.noise = noise\n",
    "        loss_vali_list = VAMP_score(chi_t_vali, chi_tau_vali, corr=corr)\n",
    "        del chi_t_vali\n",
    "        del chi_tau_vali\n",
    "        del chi_t\n",
    "        del chi_tau\n",
    "        if corr:\n",
    "            loss_vali = loss_vali_list[0].detach()\n",
    "            loss_vali_corr = loss_vali_list[1].detach()\n",
    "            epoch_loss_valid_corr[epoch] = loss_vali_corr.item()\n",
    "        else:\n",
    "            loss_vali = loss_vali_list.detach()\n",
    "        epoch_loss_valid[epoch] = loss_vali.item()\n",
    "        if best_weights_flag:\n",
    "            if epoch_loss_valid[epoch]>best_score:\n",
    "                print('Better validation score, save weights')\n",
    "                best_weights = Full_net.get_weights()\n",
    "                best_score = epoch_loss_valid[epoch]\n",
    "#         print(running_epoch_loss, running_epoch_corr)\n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        epoch_loss_corr[epoch] = np.mean(running_epoch_corr)\n",
    "        if epoch_loss_corr[epoch]>0.98:\n",
    "            weight_corr=0.\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "\n",
    "#         if (((epoch+1) % plot_mask_every)==0):\n",
    "#             plot_mask(vmax=1/n_residues*10)\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if corr:\n",
    "            plt.plot(np.arange(1,runs+1), epoch_loss_corr, label='Corr')\n",
    "            plt.plot(np.arange(1,runs+1), epoch_loss_corr, label='Corr_valid')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    if best_weights_flag:\n",
    "        print('Set best weights')\n",
    "        Full_net.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_cg(id, runs, plot_mask_every=10, verbose=True, plot_training=True):\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    \n",
    "    chi_X1_train = torch.Tensor(pred_batchwise(tensor_train_X1))\n",
    "    chi_X2_train = torch.Tensor(pred_batchwise(tensor_train_X2))\n",
    "    \n",
    "    for i in range(id):\n",
    "        chi_X1_train = Full_net.forward_cg(chi_X1_train.to(device), i)\n",
    "        chi_X2_train = Full_net.forward_cg(chi_X2_train.to(device), i)\n",
    "    \n",
    "    chi_X1_train = chi_X1_train.detach()\n",
    "    chi_X2_train = chi_X2_train.detach()\n",
    "    \n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "\n",
    "        opt = optimizer_cg[id]\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "    #     Full_net.Mask.sensitivity = sen_temp\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs_t, inputs_tau = data_batch\n",
    "\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "        chi_t = Full_net.forward_cg(chi_X1_train.to(device), id)\n",
    "        chi_tau = Full_net.forward_cg(chi_X2_train.to(device), id)\n",
    "\n",
    "#             print(chi_t)\n",
    "\n",
    "        score_curr = VAMP_score(chi_t, chi_tau)\n",
    "\n",
    "\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_epoch_loss.append(score_curr.item())\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}'.format(epoch+1, epoch_loss[epoch]))\n",
    "\n",
    "        if (((epoch+1) % plot_mask_every)==0):\n",
    "            plot_cg(id)\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.legend()\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_cg_rev(id, runs, plot_mask_every=10, verbose=True, plot_training=True):\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    \n",
    "    chi_X1_train = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "    chi_X2_train = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "    \n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_X1_train, chi_X2_train)\n",
    "    \n",
    "    matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    \n",
    "    \n",
    "    for i in range(id):\n",
    "        chi_X1_train, chi_X2_train, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                        chi_X1_train, chi_X2_train, u, S, u_t, Full_net.renorm)\n",
    "    \n",
    "    chi_X1_train = chi_X1_train.detach()\n",
    "    chi_X2_train = chi_X2_train.detach()\n",
    "    u = u.detach()\n",
    "    u_t = u_t.detach()\n",
    "    S = S.detach()\n",
    "    \n",
    "    \n",
    "#     trainset = data.TensorDataset(chi_X1_train, chi_X2_train) # create your datset\n",
    "# #     trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
    "# #                                   shuffle=True, num_workers=2)\n",
    "\n",
    "#     trainloader_full = data.DataLoader(trainset, batch_size=X1_train.shape[0],\n",
    "#                                   shuffle=True, num_workers=2)\n",
    "    \n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "\n",
    "        opt = optimizer_cg[id]\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "    #     Full_net.Mask.sensitivity = sen_temp\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs_t, inputs_tau = chi_X1_train, chi_X2_train\n",
    "\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "#             print(chi_t)\n",
    "\n",
    "        score_curr = vampe_loss_rev_cg(inputs_t, inputs_tau, u, S, u_t, id, \n",
    "                                       return_mu=False, return_mu_K_Sigma=False, renorm=Full_net.renorm)\n",
    "\n",
    "\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_epoch_loss.append(score_curr.item())\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}'.format(epoch+1, epoch_loss[epoch]))\n",
    "\n",
    "        if (((epoch+1) % plot_mask_every)==0):\n",
    "            plot_cg(id)\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of models to be trained\n",
    "runs=10\n",
    "weights_chi_list = []\n",
    "\n",
    "\n",
    "for r in range(runs):\n",
    "    \n",
    "    X1_train, X2_train, X1_vali, X2_vali, X1_test, X2_test, length_train, length_vali, _, _ = get_data_for_tau(traj_whole_new, tau_chi)\n",
    "\n",
    "    tensor_train_X1 = torch.Tensor(X1_train)\n",
    "    tensor_train_X2 = torch.Tensor(X2_train) # transform to torch tensor\n",
    "    tensor_valid_X1 = torch.Tensor(X1_vali)\n",
    "    tensor_valid_X2 = torch.Tensor(X2_vali)\n",
    "    tensor_test_X1 = torch.Tensor(X1_test)\n",
    "    tensor_test_X2 = torch.Tensor(X2_test)\n",
    "\n",
    "    trainset = data.TensorDataset(tensor_train_X1, tensor_train_X2) # create your datset\n",
    "\n",
    "    trainloader = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=batch_size, drop_last=True))\n",
    "    # trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
    "    #                               shuffle=True, num_workers=2)\n",
    "    trainloader_full = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=X1_train.shape[0], drop_last=True))\n",
    "    # trainloader_full = data.DataLoader(trainset, batch_size=batch_size_large,\n",
    "    #                               shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = data.TensorDataset(tensor_valid_X1, tensor_valid_X2) # create your datset\n",
    "    testloader = data.DataLoader(testset, sampler=data.BatchSampler(data.RandomSampler(testset), batch_size=X1_vali.shape[0], drop_last=True))\n",
    "    # testloader = data.DataLoader(testset, batch_size=batch_size,\n",
    "    #                              shuffle=True, num_workers=2)\n",
    "\n",
    "    full_batch = False\n",
    "    if full_batch:\n",
    "        train_l = trainloader_full\n",
    "    else:\n",
    "        train_l = trainloader\n",
    "\n",
    "\n",
    "    # store this so that all the data are transformed into a whitened dataset w.r.t. the same mean and std\n",
    "    # we assume a gaussian distribution of our data\n",
    "    train_mean = X1_train.mean(0)\n",
    "    train_std = X1_train.std(0)\n",
    "\n",
    "\n",
    "\n",
    "    noise=1.\n",
    "    fac_bf_sm = True\n",
    "    valid_T=True # if valid transition matrix is enforced\n",
    "    reversible=True # if reversibility is enforced\n",
    "\n",
    "    # attention stuff\n",
    "    mask_const=False # if the trained attention mask is constant over time\n",
    "    patchsize=4\n",
    "    mask_depth=4 # if time dependent how many hidden layers has the attention network\n",
    "    mask_width=layer_width # the width of the attention hidden layers\n",
    "    sensitivity=1. # factor before attention softmax for clearer assignment\n",
    "    factor_att=True # if to use a factor which scales the input on average back to input\n",
    "\n",
    "    softmax_fac=1. # factor before classification softmax\n",
    "\n",
    "\n",
    "    Full_net = VampNet(input_size, output_sizes, nodes, train_mean, train_std, \n",
    "                     valid_T=valid_T, reversible=reversible,\n",
    "                     mask_const=mask_const, mask_depth=mask_depth, mask_width=mask_width, patchsize=patchsize, sensitivity=sensitivity,\n",
    "                     fac=factor_att, noise=noise,\n",
    "                     softmax_fac=softmax_fac)\n",
    "    Full_net.to(device)\n",
    "    \n",
    "    optimizer_vamp = optim.Adam(Full_net.get_params_vamp(), learning_rate/2)\n",
    "    # opt_list = [optimizer_vamp, optimizer_vamp_mask]\n",
    "    opt_list = [optimizer_vamp]\n",
    "    train_for_VAMPnet(60, opt_list, corr=True)\n",
    "    train_for_VAMPnet(200, opt_list, corr=False, best_weights_flag=True)\n",
    "    \n",
    "    weights_chi_list.append(Full_net.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 8\n",
    "tau_msm = tau\n",
    "tau_ck = np.arange(1,(steps+1))*tau_msm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = np.concatenate([np.array([3, 5]), tau_ck])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation via ITS and CK-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag = (np.linspace(3.22,6.3, 10)**4).astype('int')\n",
    "K_results_6 = np.ones((runs, len(lag) ,output_sizes[0], output_sizes[0]))\n",
    "its_6_all_vamp = []\n",
    "for r in range(runs):\n",
    "    Full_net.set_weights(weights_chi_list[r])\n",
    "    pred = pred_batchwise(traj_whole_new[0], batchsize=10000)\n",
    "    \n",
    "    for i, tau_i in enumerate(lag):\n",
    "\n",
    "        K_results_6[r,i]  = estimate_koopman_op(pred, tau_i, force_symmetric = False)\n",
    "\n",
    "its_6_all_vamp = get_its(K_results_6, lag, False, multiple_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_its_vamp_np = np.array(its_6_all_vamp)\n",
    "all_its_vamp_mean = all_its_vamp_np.mean(0)\n",
    "all_its_vamp_min = all_its_vamp_np.min(0)\n",
    "all_its_vamp_max = all_its_vamp_np.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 200.*skip*1e-6 \n",
    "# fac = 0.0002\n",
    "\n",
    "plt.figure(figsize=(6,4));\n",
    "\n",
    "label_x = np.array([.1,0.3,1, 2, 5,10,100,1000])/fac # array is in microsecond\n",
    "label_y = np.array([.1,1, 2, 5,10, 100, 1000])/fac\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "for j in range(0,output_sizes[0]-1):\n",
    "    plt.semilogy(lag, all_its_vamp_mean[::-1][j], lw=5)\n",
    "    plt.fill_between(lag, all_its_vamp_min[::-1][j], all_its_vamp_max[::-1][j], alpha = 0.3)\n",
    "plt.semilogy(lag,lag, 'k')\n",
    "plt.xlabel('lag [$\\mu$s]', fontsize=26)\n",
    "plt.xticks(label_x, label_x*fac, fontsize=22)\n",
    "plt.ylabel('timescale [$\\mu$s]', fontsize=26)\n",
    "plt.yticks(label_y, np.round(label_y*fac, decimals=1), fontsize=22)\n",
    "plt.fill_between(lag,lag,0.1,alpha = 0.2,color='k');\n",
    "plt.ylim(0.01/fac, 3/fac)\n",
    "plt.xlim(lag[0], 0.3/fac)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_vamp = weights_chi_list[1]\n",
    "Full_net.set_weights(weights_vamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ck(K, lag):\n",
    "    n_states = output_sizes[0]\n",
    "    steps = len(lag)\n",
    "    predicted = np.zeros((n_states, n_states, steps))\n",
    "    estimated = np.zeros((n_states, n_states, steps))\n",
    "\n",
    "    predicted[:,:,0] =  np.identity(n_states)\n",
    "    estimated[:,:,0] =  np.identity(n_states)\n",
    "\n",
    "    for vector, i  in zip(np.identity(n_states), range(n_states)):\n",
    "        for n in range(1, steps):\n",
    "\n",
    "            koop = K[0]\n",
    "            fac = lag[n]//lag[0]\n",
    "            koop_pred = np.linalg.matrix_power(koop,fac)\n",
    "\n",
    "            koop_est = K[n]\n",
    "\n",
    "            predicted[i,:,n]= vector @ koop_pred\n",
    "            estimated[i,:,n]= vector @ koop_est\n",
    "        \n",
    "              \n",
    "    return [predicted, estimated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on = 10000\n",
    "pred = pred_batchwise(traj_whole_new[0], batchsize=10000)\n",
    "\n",
    "K_ck_vamp = np.zeros((runs, tau_ck.shape[0], output_sizes[0], output_sizes[0]))\n",
    "all_lx = []\n",
    "all_rx = []\n",
    "for r in range(runs):\n",
    "    for i, tau_ck_i in enumerate(tau_ck):\n",
    "        pred_t = pred[:-tau_ck_i]\n",
    "        pred_tau = pred[tau_ck_i:]\n",
    "        frames_index = np.arange(pred_t.shape[0])\n",
    "        indexes = np.random.choice(frames_index, size=test_on, replace=True)\n",
    "        ck_traj = pred_t[indexes]\n",
    "        ck_traj_tau = pred_tau[indexes]\n",
    "        K_ck_vamp[r,i] = estimate_koopman_op([ck_traj, ck_traj_tau], 0)\n",
    "    lx_side, rx_side = get_ck(K_ck_vamp[r], tau_ck)\n",
    "    all_lx.append(lx_side)\n",
    "    all_rx.append(rx_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lx_arr = np.array(all_lx)\n",
    "lx_mean = np.mean(all_lx_arr, axis=0)\n",
    "lx_min = np.min(all_lx_arr, axis=0)\n",
    "lx_max = np.max(all_lx_arr, axis=0)\n",
    "all_rx_arr = np.array(all_rx)\n",
    "rx_mean = np.mean(all_rx_arr, axis=0)\n",
    "rx_min = np.min(all_rx_arr, axis=0)\n",
    "rx_max = np.max(all_rx_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "output_size = output_sizes[0]\n",
    "fig = plt.figure(figsize = (16,16))\n",
    "gs1 = gridspec.GridSpec(output_size, output_size)\n",
    "gs1.update(wspace=0.1, hspace=0.05)\n",
    "states = output_size\n",
    "for index_i in range(states):\n",
    "    for index_j in range(states):\n",
    "        ax = plt.subplot(gs1[index_i*output_size+index_j])\n",
    "        ax.plot(tau_ck, lx_mean[index_i, index_j], color='b', lw=4)\n",
    "        ax.fill_between(tau_ck,lx_min[index_i, index_j],lx_max[index_i, index_j], alpha = 0.25 )\n",
    "        ax.errorbar(tau_ck, rx_mean[index_i, index_j], yerr= np.array([rx_mean[index_i][index_j]-rx_min[index_i][index_j], rx_max[index_i][index_j]-rx_mean[index_i][index_j]]), color = 'r', lw=4, linestyle = '--')\n",
    "        title = str(index_i+1)+ '->' +str(index_j+1)\n",
    "        \n",
    "        ax.text(.75,.8, title,\n",
    "            horizontalalignment='center',\n",
    "            transform=ax.transAxes,  fontdict = {'size':26})\n",
    "    \n",
    "        ax.set_ylim((-0.1,1.1));\n",
    "        ax.set_xlim((0, tau_ck[-1]+5));\n",
    "        \n",
    "        if (index_j == 0):\n",
    "            ax.axes.get_yaxis().set_ticks([0, 1])\n",
    "            ax.yaxis.set_tick_params(labelsize=32)\n",
    "        \n",
    "        else:\n",
    "            ax.axes.get_yaxis().set_ticks([])\n",
    "        \n",
    "        if (index_i == output_size -1):\n",
    "            \n",
    "            xticks = np.array([20,60])\n",
    "            float_formatter = lambda x: np.array([(\"%.1f\" % y if y > 0.001 else \"0\") for y in x])\n",
    "            \n",
    "            ax.xaxis.set_ticks(xticks);\n",
    "            ax.xaxis.set_ticklabels((xticks*fac));\n",
    "            ax.xaxis.set_tick_params(labelsize=32)\n",
    "        else:\n",
    "            ax.axes.get_xaxis().set_ticks([])\n",
    "            \n",
    "        if (index_i == output_size - 1 and index_j == output_size - 4):\n",
    "            ax.text(2.16, -0.4, \"[$\\mu$s]\",\n",
    "                horizontalalignment='center',\n",
    "                transform=ax.transAxes,  fontdict = {'size':28})\n",
    "\n",
    "# fig.savefig('../figs/ck_villin_states_{}_vamp.pdf'.format(states), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for negative entries in the Koopman matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_t = pred_batchwise(tensor_train_X1)\n",
    "chi_tau = pred_batchwise(tensor_train_X2)\n",
    "K_vamp = estimate_koopman_op([chi_t, chi_tau], 0)\n",
    "print(np.linalg.eigvals(K_vamp))\n",
    "print(chi_t.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_vamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a reversible deep MSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_vamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to train for u and S individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_S(runs=100, verbose=True, plot_training=True):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "    _, _, v, C_00, C_11, C_01, Sigma, _, _ = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    \n",
    "    v = v.detach()\n",
    "    C_00 = C_00.detach()\n",
    "    C_11 = C_11.detach()\n",
    "    C_01 = C_01.detach()\n",
    "    Sigma = Sigma.detach()\n",
    "    chi_t.detach()\n",
    "    chi_tau.detach()\n",
    "    \n",
    "    chi_t_valid = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device)\n",
    "    chi_tau_valid = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device)\n",
    "    _, _, v_valid, C_00_valid, C_11_valid, C_01_valid, Sigma_valid, _, _ = Full_net.u_layers[0](chi_t_valid, chi_tau_valid)\n",
    "    \n",
    "    v_valid = v_valid.detach()\n",
    "    C_00_valid = C_00_valid.detach()\n",
    "    C_11_valid = C_11_valid.detach()\n",
    "    C_01_valid = C_01_valid.detach()\n",
    "    Sigma_valid = Sigma_valid.detach()\n",
    "    chi_t_valid.detach()\n",
    "    chi_tau_valid.detach()\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "    opt = optimizer_rev_S\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "\n",
    "#             print(chi_t)\n",
    "\n",
    "        score_curr = vampe_loss_rev_only_S(v, C_00, C_11, C_01, Sigma)\n",
    "\n",
    "\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(-loss.item())\n",
    "        \n",
    "        score_curr_valid = vampe_loss_rev_only_S(v_valid, C_00_valid, C_11_valid, C_01_valid, Sigma_valid)\n",
    "        epoch_loss_valid[epoch] = np.mean(score_curr_valid.item())\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_u_S(runs=100, verbose=True, plot_training=True):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device).detach()\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device).detach()\n",
    "    \n",
    "    chi_t_valid = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "    chi_tau_valid = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "    opt = optimizer_rev_u_S\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "\n",
    "#             print(chi_t)\n",
    "        score_curr = vampe_loss_rev(chi_t, chi_tau)\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        score_curr_valid = vampe_loss_rev(chi_t_valid, chi_tau_valid)\n",
    "        epoch_loss[epoch] = np.mean(-loss.item())\n",
    "        epoch_loss_valid[epoch] = np.mean(score_curr_valid.item())\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_its(data, lags, calculate_K = True, multiple_runs = False):\n",
    "    \n",
    "    def get_single_its(data):\n",
    "\n",
    "        if type(data) == list:\n",
    "            outputsize = data[0].shape[1]\n",
    "        else:\n",
    "            outputsize = data.shape[1]\n",
    "\n",
    "        single_its = np.zeros((outputsize-1, len(lags)))\n",
    "\n",
    "        for t, tau_lag in enumerate(lags):\n",
    "            if calculate_K:\n",
    "                koopman_op = estimate_koopman_op(data, tau_lag)\n",
    "            else:\n",
    "                koopman_op = data[t]\n",
    "            k_eigvals, k_eigvec = np.linalg.eig(np.real(koopman_op))\n",
    "            k_eigvals = np.sort(np.absolute(k_eigvals))\n",
    "            k_eigvals = k_eigvals[:-1]\n",
    "            single_its[:,t] = (-tau_lag / np.log(k_eigvals))\n",
    "\n",
    "        return np.array(single_its)\n",
    "\n",
    "\n",
    "    if not multiple_runs:\n",
    "\n",
    "        its = get_single_its(data)\n",
    "\n",
    "    else:\n",
    "\n",
    "        its = []\n",
    "        for data_run in data:\n",
    "            its.append(get_single_its(data_run))\n",
    "\n",
    "    return its"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining u and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "if Full_net.reversible:\n",
    "    optimizer_rev = optim.Adam(Full_net.get_params_rev(), lr=learning_rate/10)\n",
    "    \n",
    "    optimizer_rev_S = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*1000)\n",
    "    optimizer_rev_u = optim.Adam(Full_net.get_params_rev(all=False, S_flag=False), lr=learning_rate*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize u and S with values coming from the VAMPnet\n",
    "if Full_net.reversible:\n",
    "    Full_net.set_rev_var(S=True)\n",
    "    train_for_S(runs=3000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_after_S = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_rev_u_S = optim.Adam(Full_net.get_params_rev(all=False), lr=learning_rate*10)\n",
    "optimizer_rev_S = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_after_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Full_net.reversible:\n",
    "    for _ in range(10):\n",
    "        train_for_u_S(runs=1000, verbose=False)\n",
    "        train_for_S(runs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_rev(runs, opt_chi, opt_u, opt_S, rel=0.01, reset=False, plot_mask_every=10, verbose=True, plot_training=True):\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "#     sen = np.linspace(.1,2,10)\n",
    "    opt_list = [opt_chi, opt_u, opt_S]\n",
    "    \n",
    "    best_score = 0.\n",
    "    weights_best = Full_net.get_weights()\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "#         Full_net.\n",
    "#         opt = optimizer_rev\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "#         Full_net.set_soft_fac(sen[[epoch]])\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "        \n",
    "        for i, data_batch in enumerate(trainloader_full, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_t, inputs_tau = data_batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            for opt in opt_list:\n",
    "                opt.zero_grad()\n",
    "                \n",
    "\n",
    "            # estimate weights\n",
    "            chi_t = Full_net(inputs_t[0].to(device))\n",
    "            chi_tau = Full_net(inputs_tau[0].to(device))\n",
    "\n",
    "            score_curr = vampe_loss_rev(chi_t, chi_tau)\n",
    "\n",
    "\n",
    "\n",
    "            loss = - score_curr\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            for opt in opt_list:\n",
    "                opt.step()\n",
    "            \n",
    "                    \n",
    "                \n",
    "                \n",
    "            running_epoch_loss.append(score_curr.item())\n",
    "        flag=True\n",
    "        print(running_epoch_loss)\n",
    "        score_before=np.mean(running_epoch_loss)\n",
    "        while flag:\n",
    "            \n",
    "            opt_S.zero_grad()\n",
    "            opt_u.zero_grad()\n",
    "            if reset:\n",
    "                Full_net.set_rev_var(S=False)\n",
    "            chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "            chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "\n",
    "            score_curr = vampe_loss_rev(chi_t, chi_tau)\n",
    "            loss = -score_curr\n",
    "            loss.backward()\n",
    "            \n",
    "            opt_S.step()\n",
    "            opt_u.step()\n",
    "            print(score_before, score_curr)\n",
    "            if score_curr-score_before < rel:\n",
    "                flag = False\n",
    "            score_before = score_curr\n",
    "        # validation\n",
    "        chi_t_valid = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "        chi_tau_valid = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "        score_valid = vampe_loss_rev(chi_t_valid, chi_tau_valid)\n",
    "        \n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        epoch_loss_valid[epoch] = np.mean(score_valid.item())\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "        if epoch_loss_valid[epoch]> best_score:\n",
    "            best_score = epoch_loss_valid[epoch]\n",
    "            weights_best = Full_net.get_weights()\n",
    "            print('better weights')\n",
    "#         if (((epoch+1) % plot_mask_every)==0):\n",
    "#             plot_mask(vmax=1/n_residues*10)\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()  \n",
    "        \n",
    "    print('Set best weights')\n",
    "    Full_net.set_weights(weights_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_before_rec = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = optim.Adam(Full_net.get_params_vamp(), lr=learning_rate/5,) \n",
    "opt2 = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*10)\n",
    "opt3 = optim.Adam(Full_net.get_params_rev(all=False, S_flag=False), lr=learning_rate*1) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_before_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask( vmax=1/n_residues * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_t = pred_batchwise(tensor_train_X1)\n",
    "chi_tau = pred_batchwise(tensor_train_X2)\n",
    "K_vamp = estimate_koopman_op([chi_t, chi_tau], 0)\n",
    "print(np.linalg.eigvals(K_vamp))\n",
    "print(chi_t.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_vamp = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_vamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train first without resetting u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_rev(100, opt1, opt2, opt3, rel=0.0001, reset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afterwards reset u to check if the result was stuck in an suboptimal region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_rev_var(S=False)\n",
    "train_for_rev(100, opt1, opt2, opt3, rel=0.0001, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_before_rec = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_before_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K_rev(tensor_t=torch.Tensor(traj_whole_new[0][:-tau]), tensor_tau=torch.Tensor(traj_whole_new[0][tau:])):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_t)).to(device)\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_tau)).to(device)\n",
    "    _,_,v, C_00, C_11, C_01, Sigma, _, _ = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    _, K, _ = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    \n",
    "    return K.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_rev = get_K_rev().to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the eigenvalues for VAMPnet and revDMSM case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigenvalues of the VAMPnet will be higher since there are no restriction for the eigenfunctions!\n",
    "np.linalg.eigvals(K_rev), np.linalg.eigvals(K_vamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_after_rec = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate K_revs for timescales and cktest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K_results_6_rev = np.ones((runs, len(lag) ,output_sizes[0], output_sizes[0]))\n",
    "\n",
    "for r in range(runs):    \n",
    "    \n",
    "    for i, tau_i in enumerate(lag):\n",
    "        print(r, i, tau_i)\n",
    "        X1_train, X2_train, X1_vali, X2_vali, X1_test, X2_test, length_train, length_vali, _, _ = get_data_for_tau(traj_whole_new, tau_i)\n",
    "\n",
    "        tensor_train_X1 = torch.Tensor(X1_train)\n",
    "        tensor_train_X2 = torch.Tensor(X2_train) # transform to torch tensor\n",
    "        tensor_valid_X1 = torch.Tensor(X1_vali)\n",
    "        tensor_valid_X2 = torch.Tensor(X2_vali)\n",
    "        tensor_test_X1 = torch.Tensor(X1_test)\n",
    "        tensor_test_X2 = torch.Tensor(X2_test)\n",
    "\n",
    "        trainset = data.TensorDataset(tensor_train_X1, tensor_train_X2) # create your datset\n",
    "\n",
    "        trainloader = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=batch_size, drop_last=True))\n",
    "        # trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
    "        #                               shuffle=True, num_workers=2)\n",
    "        trainloader_full = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=X1_train.shape[0], drop_last=True))\n",
    "        # trainloader_full = data.DataLoader(trainset, batch_size=batch_size_large,\n",
    "        #                               shuffle=True, num_workers=2)\n",
    "\n",
    "        testset = data.TensorDataset(tensor_valid_X1, tensor_valid_X2) # create your datset\n",
    "        testloader = data.DataLoader(testset, sampler=data.BatchSampler(data.RandomSampler(testset), batch_size=X1_vali.shape[0], drop_last=True))\n",
    "        # testloader = data.DataLoader(testset, batch_size=batch_size,\n",
    "        #                              shuffle=True, num_workers=2)\n",
    "\n",
    "        full_batch = False\n",
    "        if full_batch:\n",
    "            train_l = trainloader_full\n",
    "        else:\n",
    "            train_l = trainloader\n",
    "        \n",
    "        Full_net.set_weights(weights_after_rec)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in Full_net.S_layers[0].parameters():\n",
    "                param.copy_(torch.Tensor(np.ones((output_sizes[0], output_sizes[0]))))\n",
    "#         with torch.no_grad():\n",
    "#             for param in Full_net.u_layers[0].parameters():\n",
    "#                 param.copy_(torch.Tensor(np.ones((1, output_sizes[0]))))\n",
    "\n",
    "        optimizer_rev_u_S = optim.Adam(Full_net.get_params_rev(all=False), lr=learning_rate*10)\n",
    "        optimizer_rev_S = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*100)\n",
    "        for _ in range(10):\n",
    "            train_for_S(runs=1000, verbose=False, plot_training=True)\n",
    "\n",
    "            train_for_u_S(runs=1000, verbose=False, plot_training=True)\n",
    "    #     print(tau_i)\n",
    "    #     K_results_rev[i]= training_for_tau_both(tau_i)\n",
    "        K_results_6_rev[r,i]  = get_K_rev(tensor_test_X1, tensor_test_X2).to('cpu')\n",
    "\n",
    "its_6_all_rev = get_its(K_results_6_rev, lag, False, multiple_runs=True)\n",
    "\n",
    "K_ck_rev = K_results_6_rev[:,2:]\n",
    "all_lx_rev = []\n",
    "all_rx_rev = []\n",
    "for r in range(runs):\n",
    "    lx_side, rx_side = get_ck(K_ck_rev[r], tau_ck)\n",
    "    all_lx_rev.append(lx_side)\n",
    "    all_rx_rev.append(rx_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_its_rev_np = np.array(its_6_all_rev)\n",
    "all_its_rev_mean = all_its_rev_np.mean(0)\n",
    "all_its_rev_min = all_its_rev_np.min(0)\n",
    "all_its_rev_max = all_its_rev_np.max(0)\n",
    "fac = 200.*skip*1e-6 \n",
    "# fac = 0.0002\n",
    "\n",
    "plt.figure(figsize=(6,4));\n",
    "\n",
    "label_x = np.array([.1,0.3,1, 2, 5,10,100,1000])/fac # array is in microsecond\n",
    "label_y = np.array([.1,1, 2, 5,10, 100, 1000])/fac\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "for j in range(0,output_sizes[0]-1):\n",
    "    plt.semilogy(lag, all_its_rev_mean[::-1][j], lw=5)\n",
    "    plt.fill_between(lag, all_its_rev_min[::-1][j], all_its_rev_max[::-1][j], alpha = 0.3)\n",
    "plt.semilogy(lag,lag, 'k')\n",
    "plt.xlabel('lag [$\\mu$s]', fontsize=26)\n",
    "plt.xticks(label_x, label_x*fac, fontsize=22)\n",
    "plt.ylabel('timescale [$\\mu$s]', fontsize=26)\n",
    "plt.yticks(label_y, np.round(label_y*fac, decimals=1), fontsize=22)\n",
    "plt.fill_between(lag,lag,0.1,alpha = 0.2,color='k');\n",
    "plt.ylim(0.01/fac, 3/fac)\n",
    "plt.xlim(lag[0], 0.3/fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lx_rev_arr = np.array(all_lx_rev)\n",
    "lx_rev_mean = np.mean(all_lx_rev_arr, axis=0)\n",
    "lx_rev_min = np.min(all_lx_rev_arr, axis=0)\n",
    "lx_rev_max = np.max(all_lx_rev_arr, axis=0)\n",
    "all_rx_rev_arr = np.array(all_rx_rev)\n",
    "rx_rev_mean = np.mean(all_rx_rev_arr, axis=0)\n",
    "rx_rev_min = np.min(all_rx_rev_arr, axis=0)\n",
    "rx_rev_max = np.max(all_rx_rev_arr, axis=0)\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "output_size = output_sizes[0]\n",
    "fig = plt.figure(figsize = (16,16))\n",
    "gs1 = gridspec.GridSpec(output_size, output_size)\n",
    "gs1.update(wspace=0.1, hspace=0.05)\n",
    "states = output_size\n",
    "for index_i in range(states):\n",
    "    for index_j in range(states):\n",
    "        ax = plt.subplot(gs1[index_i*output_size+index_j])\n",
    "        ax.plot(tau_ck, lx_rev_mean[index_i, index_j], color='b', lw=4)\n",
    "        ax.fill_between(tau_ck,lx_rev_min[index_i, index_j],lx_rev_max[index_i, index_j], alpha = 0.25 )\n",
    "        ax.errorbar(tau_ck, rx_rev_mean[index_i, index_j], yerr= np.array([rx_rev_mean[index_i][index_j]-rx_rev_min[index_i][index_j], rx_rev_max[index_i][index_j]-rx_rev_mean[index_i][index_j]]), color = 'r', lw=4, linestyle = '--')\n",
    "        title = str(index_i+1)+ '->' +str(index_j+1)\n",
    "        \n",
    "        ax.text(.75,.8, title,\n",
    "            horizontalalignment='center',\n",
    "            transform=ax.transAxes,  fontdict = {'size':26})\n",
    "    \n",
    "        ax.set_ylim((-0.1,1.1));\n",
    "        ax.set_xlim((0, tau_ck[-1]+5));\n",
    "        \n",
    "        if (index_j == 0):\n",
    "            ax.axes.get_yaxis().set_ticks([0, 1])\n",
    "            ax.yaxis.set_tick_params(labelsize=32)\n",
    "        \n",
    "        else:\n",
    "            ax.axes.get_yaxis().set_ticks([])\n",
    "        \n",
    "        if (index_i == output_size -1):\n",
    "            \n",
    "            xticks = np.array([20,60])\n",
    "            float_formatter = lambda x: np.array([(\"%.1f\" % y if y > 0.001 else \"0\") for y in x])\n",
    "            \n",
    "            ax.xaxis.set_ticks(xticks);\n",
    "            ax.xaxis.set_ticklabels((xticks*fac));\n",
    "            ax.xaxis.set_tick_params(labelsize=32)\n",
    "        else:\n",
    "            ax.axes.get_xaxis().set_ticks([])\n",
    "            \n",
    "        if (index_i == output_size - 1 and index_j == output_size - 4):\n",
    "            ax.text(2.16, -0.4, \"[$\\mu$s]\",\n",
    "                horizontalalignment='center',\n",
    "                transform=ax.transAxes,  fontdict = {'size':28})\n",
    "\n",
    "# fig.savefig('../figs/ck_villin_states_{}_vamp.pdf'.format(states), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the network graphs for VAMPnet and revDMSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvec = np.linalg.eig(K_vamp.T)\n",
    "sort_ind = np.argsort(eigvals)[::-1]\n",
    "pi_vamp = eigvec[:,sort_ind[1]]\n",
    "pi_vamp = pi_vamp/pi_vamp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvec = np.linalg.eig(K_rev.T)\n",
    "sort_ind = np.argsort(eigvals)[::-1]\n",
    "pi_rev = eigvec[:,sort_ind[0]]\n",
    "pi_rev = pi_rev/pi_rev.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0.25*np.array([[0.,0.],[1.,0.],[0.,1.], [1.,1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels have to adapted for specific user case\n",
    "state_labels = ['F', 'U', 'M', 'PF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_vamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, pos = networks.plot_network(K_vamp, state_sizes=pi_rev, pos = pos, arrow_label_format='%2.4f',\n",
    "                                 state_scale=1., arrow_curvature=1, state_labels=state_labels,\n",
    "                     arrow_scale=2., state_colors='lightsalmon', arrow_threshold=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, pos = networks.plot_network(K_rev.numpy(), state_sizes=pi_rev, pos = pos, arrow_label_format='%2.4f',\n",
    "                                 state_scale=1., arrow_curvature=1, state_labels=state_labels,\n",
    "                     arrow_scale=2., state_colors='lightsalmon', arrow_threshold=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save structures for defined states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has to be modified depending where the data lies for the user\n",
    "import mdtraj as md\n",
    "number_of_frames = 20\n",
    "md_traj_small = md.load_dcd(root+'-000.dcd',\n",
    "                      top=root+'.pdb'\n",
    "                )[:number_of_frames]\n",
    "md_traj_super = md.load_pdb('../../dmsm_recap/vampnets/folded_states/2f4k_villin.pdb')\n",
    "# number of residues for the system\n",
    "n_residues=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate secondary structure for allignment\n",
    "def get_heavy_atoms_dssp(dssp, in_all=True):\n",
    "    list_atoms = []\n",
    "    nd_s = ['H', 'E']\n",
    "    \n",
    "    if in_all:\n",
    "        frames_top = dssp.shape[0]\n",
    "    else:\n",
    "        frames_top = 1\n",
    "    \n",
    "        \n",
    "    list_res2nd = [True]*n_residues\n",
    "    for dssp_frame in dssp[:frames_top]:\n",
    "        for res in range(n_residues):\n",
    "            if dssp_frame[res] not in nd_s:\n",
    "                    list_res2nd[res] = False\n",
    "                    \n",
    "\n",
    "    for res in range(n_residues):\n",
    "        if list_res2nd[res]:\n",
    "            atoms_ind = [atom.index for atom in md_traj_super.topology.atoms if atom.residue == md_traj_super.topology.residue(res) and atom.name != 'H']\n",
    "            list_atoms+= atoms_ind\n",
    "    return list_atoms, list_res2nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "dssp_super = md.compute_dssp(md_traj_super, simplified=True)[:,:n_residues]\n",
    "index_heavy_2nd = get_heavy_atoms_dssp(dssp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structures(cg_ind, vmax=1, skip=10, top=10):\n",
    "    n_residues = int(-1/2 + np.sqrt(1/4+input_size*2) + 3)\n",
    "    if Full_net.mask_const:\n",
    "        attention = Full_net.get_attention()\n",
    "        attention_np = attention.detach().numpy()\n",
    "        att_atom = np.reshape(attention_np, (n_residues,1))\n",
    "        plt.imshow(att_atom, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('System', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(1),['{}'.format(i) for i in range(1)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return att_atom\n",
    "        \n",
    "    else:\n",
    "        pred_temp = pred_batchwise(traj_whole[0], batchsize=10000)\n",
    "        pred_temp = torch.Tensor(pred_temp).to(device)\n",
    "        for i in range(cg_ind):\n",
    "            pred_temp = Full_net.forward_cg(pred_temp, i)\n",
    "        pred_temp = pred_temp.detach().to('cpu').numpy()\n",
    "        print(pred_temp.shape)\n",
    "        arg_sort = np.argsort(pred_temp, axis=0)\n",
    "        top_x_state = arg_sort[-top:][::-1]\n",
    "        states = pred_temp.shape[1]\n",
    "        att_atom = []\n",
    "        for state in range(states):\n",
    "            frames = top_x_state[:,state]\n",
    "            attention = Full_net.get_attention(torch.Tensor(traj_whole[0][frames]).to(device))\n",
    "            attention_np = attention.detach().to('cpu').numpy()\n",
    "#             att_atom.append(np.mean(attention_np, axis=0, keepdims=True))\n",
    "            print(attention_np.shape)\n",
    "            att_atom.append(attention_np[None,:,:])\n",
    "        att_residue = np.concatenate(att_atom, axis=0)\n",
    "        \n",
    "        plt.imshow(np.mean(att_residue,axis=1).T, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('State', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(states),['{}'.format(i) for i in range(states)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return att_residue, top_x_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_after_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save structures for revDMSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_nr=0\n",
    "threshold = 0\n",
    "out_ind=0\n",
    "attention, frames = structures(out_ind, top=number_of_frames, vmax=3)\n",
    "for o_temp in range(frames.shape[1]):\n",
    "    \n",
    "    o = o_temp\n",
    "    print(o)\n",
    "    attention_clean = np.zeros_like(attention[o])\n",
    "    for f in range(number_of_frames):\n",
    "        for i in range(n_residues):\n",
    "            if attention[o,f,i] < threshold:\n",
    "                attention_clean[f,i]=0\n",
    "            else:\n",
    "                attention_clean[f,i]=attention[o,f,i] \n",
    "    attention_fixed = np.concatenate([attention_clean, attention_clean], axis=1)\n",
    "    bfactors = np.repeat(attention_fixed, [res.n_atoms for res in md_traj_super.top.residues], axis=1)\n",
    "\n",
    "    frames_of_state = frames[:,o]\n",
    "    frames_per_file = 10000\n",
    "    for i in range(number_of_frames):\n",
    "        file_number = frames_of_state[i]//frames_per_file\n",
    "        file_number_frame = frames_of_state[i]%frames_per_file\n",
    "    #             print(numbers_all[i], traj_nr[i], file_number, file_number_frame)\n",
    "        root_new = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-{1}-protein/{0}-{1}-protein/{0}-{1}-protein'.format(test_system, traj_nr)\n",
    "        md_traj_start = md.load_dcd(root_new+'-{:03}.dcd'.format(file_number),\n",
    "                              top='/group/ag_cmb/scratch/deeptime_data/{0}/system-protein.pdb'.format(test_system)\n",
    "                        )\n",
    "        md_traj_small.xyz[i] = md_traj_start.xyz[file_number_frame]\n",
    "\n",
    "    md_traj_small.superpose(md_traj_super) \n",
    "    dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "\n",
    "    index_heavy_2nd, list_res = get_heavy_atoms_dssp(dssp)\n",
    "    print('Index of residues in 2nd structure', np.arange(n_residues)[list_res])\n",
    "    if index_heavy_2nd:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=index_heavy_2nd)\n",
    "        print('found secondary structure and aligning with {} atoms'.format(len(index_heavy_2nd)))\n",
    "    else:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=None)\n",
    "        print('did not find secondary structure and aligning with pdb {} atoms'.format(len(index_heavy_2nd)))\n",
    "\n",
    "    md_traj_small_temp.save_pdb('/group/ag_cmb/scratch/deeptime_data/{0}/attention/rev_{1}_{2}_smooth{3}.pdb'.format(test_system, output_sizes[out_ind], o_temp, patchsize, test), bfactors=bfactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_vamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save structures for VAMPnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_nr=0\n",
    "threshold = 0\n",
    "out_ind=0\n",
    "attention, frames = structures(out_ind, top=number_of_frames, vmax=3)\n",
    "for o_temp in range(frames.shape[1]):\n",
    "    \n",
    "    o = o_temp\n",
    "    print(o)\n",
    "    attention_clean = np.zeros_like(attention[o])\n",
    "    for f in range(number_of_frames):\n",
    "        for i in range(n_residues):\n",
    "            if attention[o,f,i] < threshold:\n",
    "                attention_clean[f,i]=0\n",
    "            else:\n",
    "                attention_clean[f,i]=attention[o,f,i] \n",
    "    attention_fixed = np.concatenate([attention_clean, attention_clean], axis=1)\n",
    "    bfactors = np.repeat(attention_fixed, [res.n_atoms for res in md_traj_super.top.residues], axis=1)\n",
    "\n",
    "    frames_of_state = frames[:,o]\n",
    "    frames_per_file = 10000\n",
    "    for i in range(number_of_frames):\n",
    "        file_number = frames_of_state[i]//frames_per_file\n",
    "        file_number_frame = frames_of_state[i]%frames_per_file\n",
    "    #             print(numbers_all[i], traj_nr[i], file_number, file_number_frame)\n",
    "        root_new = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-{1}-protein/{0}-{1}-protein/{0}-{1}-protein'.format(test_system, traj_nr)\n",
    "        md_traj_start = md.load_dcd(root_new+'-{:03}.dcd'.format(file_number),\n",
    "                              top='/group/ag_cmb/scratch/deeptime_data/{0}/system-protein.pdb'.format(test_system)\n",
    "                        )\n",
    "        md_traj_small.xyz[i] = md_traj_start.xyz[file_number_frame]\n",
    "\n",
    "    md_traj_small.superpose(md_traj_super) \n",
    "    dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "\n",
    "    index_heavy_2nd, list_res = get_heavy_atoms_dssp(dssp)\n",
    "    print('Index of residues in 2nd structure', np.arange(n_residues)[list_res])\n",
    "    if index_heavy_2nd:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=index_heavy_2nd)\n",
    "        print('found secondary structure and aligning with {} atoms'.format(len(index_heavy_2nd)))\n",
    "    else:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=None)\n",
    "        print('did not find secondary structure and aligning with pdb {} atoms'.format(len(index_heavy_2nd)))\n",
    "\n",
    "    md_traj_small_temp.save_pdb('/group/ag_cmb/scratch/deeptime_data/{0}/attention/vamp_{1}_{2}_smooth{3}.pdb'.format(test_system, output_sizes[out_ind], o_temp, patchsize, test), bfactors=bfactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_after_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train revDMSM for different tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train, X1_vali, X2_vali, X1_test, X2_test, length_train, length_vali, _, _ = get_data_for_tau(traj_whole_new, tau)\n",
    "\n",
    "tensor_train_X1 = torch.Tensor(X1_train)\n",
    "tensor_train_X2 = torch.Tensor(X2_train) # transform to torch tensor\n",
    "tensor_valid_X1 = torch.Tensor(X1_vali)\n",
    "tensor_valid_X2 = torch.Tensor(X2_vali)\n",
    "tensor_test_X1 = torch.Tensor(X1_test)\n",
    "tensor_test_X2 = torch.Tensor(X2_test)\n",
    "\n",
    "trainset = data.TensorDataset(tensor_train_X1, tensor_train_X2) # create your datset\n",
    "\n",
    "trainloader = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=batch_size, drop_last=True))\n",
    "# trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                               shuffle=True, num_workers=2)\n",
    "trainloader_full = data.DataLoader(trainset, sampler=data.BatchSampler(data.RandomSampler(trainset), batch_size=batch_size_large, drop_last=True))\n",
    "# trainloader_full = data.DataLoader(trainset, batch_size=batch_size_large,\n",
    "#                               shuffle=True, num_workers=2)\n",
    "\n",
    "testset = data.TensorDataset(tensor_valid_X1, tensor_valid_X2) # create your datset\n",
    "testloader = data.DataLoader(testset, sampler=data.BatchSampler(data.RandomSampler(testset), batch_size=batch_size, drop_last=True))\n",
    "# testloader = data.DataLoader(testset, batch_size=batch_size,\n",
    "#                              shuffle=True, num_workers=2)\n",
    "\n",
    "full_batch = False\n",
    "if full_batch:\n",
    "    train_l = trainloader_full\n",
    "else:\n",
    "    train_l = trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain for the new tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_after_rec)\n",
    "with torch.no_grad():\n",
    "    for param in Full_net.S_layers[0].parameters():\n",
    "        param.copy_(torch.Tensor(np.ones((output_sizes[0], output_sizes[0]))))\n",
    "with torch.no_grad():\n",
    "    for param in Full_net.u_layers[0].parameters():\n",
    "        param.copy_(torch.Tensor(np.ones((1, output_sizes[0]))))\n",
    "        \n",
    "optimizer_rev_u_S = optim.Adam(Full_net.get_params_rev(all=False), lr=learning_rate*10)\n",
    "optimizer_rev_S = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*100)\n",
    "Full_net.set_rev_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    train_for_S(runs=1000, verbose=False, plot_training=True)\n",
    "\n",
    "    train_for_u_S(runs=1000, verbose=False, plot_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare again with the VAMPnet, the difference should become smaller with larger tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_msm = get_K_rev(tensor_test_X1, tensor_test_X2).to('cpu')\n",
    "chi_t = pred_batchwise(tensor_test_X1)\n",
    "chi_tau = pred_batchwise(tensor_test_X2)\n",
    "K_vamp_msm = estimate_koopman_op([chi_t, chi_tau], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(K_msm), np.linalg.eigvals(K_vamp_msm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_msm = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_msm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate eigenfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.Mask.noise=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_t = torch.Tensor(traj_whole[0][:-tau*skip])\n",
    "tensor_tau = torch.Tensor(traj_whole[0][tau*skip:])\n",
    "K_msm = get_K_rev(tensor_t, tensor_tau).to('cpu')\n",
    "chi_t = pred_batchwise(tensor_t)\n",
    "chi_tau = pred_batchwise(tensor_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvec = np.linalg.eig(K_msm)\n",
    "print(eigvals, eigvec)\n",
    "sort_id = np.argsort(eigvals)[::-1]\n",
    "print(eigvals[sort_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp_all = []\n",
    "dssp_all_high = []\n",
    "for dcc_traj in range(63):\n",
    "    if dcc_traj<10:\n",
    "        s = '-00{}.dcd'.format(dcc_traj)\n",
    "    else:\n",
    "        s = '-0{}.dcd'.format(dcc_traj)\n",
    "    md_temp = md.load_dcd(root+s,\n",
    "                         top=root+'.pdb')[::1]\n",
    "    dssp_temp = md.compute_dssp(md_temp, simplified=True)[:,:n_residues]\n",
    "    dssp_all.append(dssp_temp)\n",
    "    dssp_temp = md.compute_dssp(md_temp, simplified=False)[:,:n_residues]\n",
    "    dssp_all_high.append(dssp_temp)\n",
    "dssp_all_np = np.concatenate(dssp_all, axis=0)\n",
    "dssp_all_high_np = np.concatenate(dssp_all_high, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ord = pred_batchwise(traj_whole[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include only secondary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ind = [2,13,21]\n",
    "end_ind = [12,20,33]\n",
    "not_native_ind = []\n",
    "not_native_ind.append(np.arange(start_ind[0]))\n",
    "native_ind = []\n",
    "\n",
    "for i in range(2):\n",
    "    not_native_ind.append(np.arange(end_ind[i],start_ind[i+1]))\n",
    "    native_ind.append(np.arange(start_ind[i],end_ind[i]))\n",
    "    if i ==1:\n",
    "        native_ind.append(np.arange(start_ind[i+1],end_ind[i+1]))\n",
    "not_native_ind.append(np.arange(end_ind[-1],n_residues))\n",
    "not_native_ind = np.concatenate(not_native_ind)\n",
    "native_ind = np.concatenate(native_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp_super = md.compute_dssp(md_traj_super, simplified=True)[:,:n_residues]\n",
    "dssp_super_high = md.compute_dssp(md_traj_super, simplified=False)[:,:n_residues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = 25\n",
    "possible_not_native = len(not_native_ind)\n",
    "possible_native = len(native_ind)\n",
    "dssp_frame_native = (dssp_all_high_np[:,native_ind] == dssp_super_high[:,native_ind]).sum(1)/possible_native\n",
    "dssp_frame_non = (dssp_all_np[:,not_native_ind] != 'C').sum(1)/possible_not_native\n",
    "for state_i in range(2):\n",
    "    ind = sort_id[1+state_i]\n",
    "    print('Eigenfunction {} of total states {} with eigenvalue {:.3}'.format(state_i, output_sizes[0], eigvals[ind]))\n",
    "\n",
    "        \n",
    "    eigfunc = pred_ord @ eigvec[:,ind]\n",
    "    sort_ind = np.argsort(eigfunc)[::sk]\n",
    "    plt.plot(eigfunc[sort_ind], '.')\n",
    "#     plt.plot(eigfunc[::sk], dssp_frame_non[::sk], '.')\n",
    "    plt.xlabel('$\\psi_{}$ [a.u.]'.format(state_i+1), fontsize=14)\n",
    "    plt.ylabel('Ratio of native helical residues', fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.plot(dssp_frame[sort_ind[::10]], '.')\n",
    "    plt.plot(moving_average(dssp_frame_native[sort_ind],30), '.')\n",
    "    plt.xlabel('$\\psi_{}$ [a.u.]'.format(state_i+1), fontsize=14)\n",
    "    plt.ylabel('Ratio of native helical residues', fontsize=14)\n",
    "#     plt.plot(dssp_frame_non[sort_ind[::10]], '.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slowest eigenfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=0\n",
    "native_contacts_conc = dssp_frame_native\n",
    "ind = sort_id[1+state]\n",
    "print('Eigenfunction {} of total states {} with eigenvalue {:.3}'.format(state_i, output_sizes[0], eigvals[ind]))\n",
    "eigfunc = pred_ord @ eigvec[:,ind]\n",
    "\n",
    "sort_ind = np.argsort(eigfunc)\n",
    "ave_size = 30\n",
    "\n",
    "sort_ind_sk = sort_ind[::sk]\n",
    "add = 0.05\n",
    "argmin2 = np.argmin((eigfunc-add)**2)\n",
    "\n",
    "argmin1 = np.argmin((eigfunc+add)**2)\n",
    "\n",
    "\n",
    "plt.plot(eigfunc[sort_ind_sk], '.')\n",
    "\n",
    "plt.vlines(np.argwhere(sort_ind==argmin2)//sk,eigfunc.min(),eigfunc.max(), colors='C1', zorder=3)\n",
    "plt.vlines(np.argwhere(sort_ind==argmin1)//sk,eigfunc.min(),eigfunc.max(), colors='C2', zorder=3)\n",
    "rgwhere(sort_ind_sk==argmin)+10, 0, '$\\psi=0.2$')\n",
    "plt.xlabel('Configuration ordered by $\\psi_1$', fontsize=16)\n",
    "plt.ylabel('$\\psi_1$ [a.u.]', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "# plt.savefig('./First_eigenfunction_villin_ordered_explain.png',dpi=1000, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(np.arange(eigfunc[::sk].shape[0]-ave_size+1),moving_average(native_contacts_conc[sort_ind_sk],ave_size), c=eigfunc[sort_ind_sk][:-ave_size+1], cmap='plasma')\n",
    "plt.vlines(np.argwhere(sort_ind==argmin2)//sk,0,1, colors='C1', zorder=3)\n",
    "plt.vlines(np.argwhere(sort_ind==argmin1)//sk,0,1, colors='C2', zorder=3)\n",
    "plt.xlabel('Configuration ordered by $\\psi_1$', fontsize=16)\n",
    "plt.ylabel('Native contacts ratio', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "# plt.savefig('./First_eigenfunction_villin_ordered_colored.png',dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_eig = []\n",
    "mean_12 = np.argwhere(sort_ind==argmin1)+(np.argwhere(sort_ind==argmin2) - np.argwhere(sort_ind==argmin1))//2\n",
    "# mean_23 = np.argwhere(sort_ind==argmin2)+(np.argwhere(sort_ind==argmin3) - np.argwhere(sort_ind==argmin2))//2\n",
    "frames_eig.append(sort_ind[0:20])\n",
    "frames_eig.append(sort_ind[mean_12[0,0]-10:mean_12[0,0]+10])\n",
    "# frames_eig.append(sort_ind[mean_23[0,0]-5:mean_23[0,0]+5])\n",
    "frames_eig.append(sort_ind[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_nr=0\n",
    "threshold = 0\n",
    "out_ind=0\n",
    "\n",
    "for o_temp in range(len(frames_eig)):\n",
    "    \n",
    "    o = o_temp\n",
    "    print(o)\n",
    "    \n",
    "\n",
    "    frames_of_state = frames_eig[o]\n",
    "    frames_per_file = 10000\n",
    "    for i in range(number_of_frames):\n",
    "        file_number = frames_of_state[i]//frames_per_file\n",
    "        file_number_frame = frames_of_state[i]%frames_per_file\n",
    "    #             print(numbers_all[i], traj_nr[i], file_number, file_number_frame)\n",
    "        root_new = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-{1}-protein/{0}-{1}-protein/{0}-{1}-protein'.format(test_system, traj_nr)\n",
    "        md_traj_start = md.load_dcd(root_new+'-{:03}.dcd'.format(file_number),\n",
    "                              top='/group/ag_cmb/scratch/deeptime_data/{0}/system-protein.pdb'.format(test_system)\n",
    "                        )\n",
    "        md_traj_small.xyz[i] = md_traj_start.xyz[file_number_frame]\n",
    "\n",
    "    md_traj_small.superpose(md_traj_super) \n",
    "    dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "\n",
    "    index_heavy_2nd, list_res = get_heavy_atoms_dssp(dssp)\n",
    "    print('Index of residues in 2nd structure', np.arange(n_residues)[list_res])\n",
    "    if index_heavy_2nd:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=index_heavy_2nd)\n",
    "        print('found secondary structure and aligning with {} atoms'.format(len(index_heavy_2nd)))\n",
    "    else:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=None)\n",
    "        print('did not find secondary structure and aligning with pdb {} atoms'.format(len(index_heavy_2nd)))\n",
    "\n",
    "    md_traj_small_temp.save_pdb('/group/ag_cmb/scratch/deeptime_data/{0}/attention/eig1_{1}_{2}_smooth{3}.pdb'.format(test_system, output_sizes[out_ind], o_temp, patchsize, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second slowest eigenfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=1\n",
    "native_contacts_conc = dssp_frame_native\n",
    "ind = sort_id[1+state]\n",
    "print('Eigenfunction {} of total states {} with eigenvalue {:.3}'.format(state_i, output_sizes[0], eigvals[ind]))\n",
    "eigfunc = pred_ord @ eigvec[:,ind]\n",
    "\n",
    "sort_ind = np.argsort(eigfunc)\n",
    "ave_size = 30\n",
    "\n",
    "sort_ind_sk = sort_ind[::sk]\n",
    "add = 0.1\n",
    "argmin2 = np.argmin((eigfunc-add)**2)\n",
    "\n",
    "argmin1 = np.argmin((eigfunc+add)**2)\n",
    "\n",
    "\n",
    "plt.plot(eigfunc[sort_ind_sk], '.')\n",
    "\n",
    "plt.vlines(np.argwhere(sort_ind==argmin2)//sk,eigfunc.min(),eigfunc.max(), colors='C1', zorder=3)\n",
    "plt.vlines(np.argwhere(sort_ind==argmin1)//sk,eigfunc.min(),eigfunc.max(), colors='C2', zorder=3)\n",
    "\n",
    "plt.xlabel('Configuration ordered by $\\psi_2$', fontsize=16)\n",
    "plt.ylabel('$\\psi_2$ [a.u.]', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "# plt.savefig('./First_eigenfunction_villin_ordered_explain.png',dpi=1000, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(np.arange(eigfunc[::sk].shape[0]-ave_size+1),moving_average(native_contacts_conc[sort_ind_sk],ave_size), c=eigfunc[sort_ind_sk][:-ave_size+1], cmap='plasma')\n",
    "plt.vlines(np.argwhere(sort_ind==argmin2)//sk,0,1, colors='C1', zorder=3)\n",
    "plt.vlines(np.argwhere(sort_ind==argmin1)//sk,0,1, colors='C2', zorder=3)\n",
    "plt.xlabel('Configuration ordered by $\\psi_2$', fontsize=16)\n",
    "plt.ylabel('Native contacts ratio', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "# plt.savefig('./First_eigenfunction_villin_ordered_colored.png',dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_eig = []\n",
    "mean_12 = np.argwhere(sort_ind==argmin1)+(np.argwhere(sort_ind==argmin2) - np.argwhere(sort_ind==argmin1))//2\n",
    "# mean_23 = np.argwhere(sort_ind==argmin2)+(np.argwhere(sort_ind==argmin3) - np.argwhere(sort_ind==argmin2))//2\n",
    "frames_eig.append(sort_ind[0:20])\n",
    "frames_eig.append(sort_ind[mean_12[0,0]-10:mean_12[0,0]+10])\n",
    "# frames_eig.append(sort_ind[mean_23[0,0]-5:mean_23[0,0]+5])\n",
    "frames_eig.append(sort_ind[-20:])\n",
    "traj_nr=0\n",
    "threshold = 0\n",
    "out_ind=0\n",
    "\n",
    "for o_temp in range(len(frames_eig)):\n",
    "    \n",
    "    o = o_temp\n",
    "    print(o)\n",
    "    \n",
    "\n",
    "    frames_of_state = frames_eig[o]\n",
    "    frames_per_file = 10000\n",
    "    for i in range(number_of_frames):\n",
    "        file_number = frames_of_state[i]//frames_per_file\n",
    "        file_number_frame = frames_of_state[i]%frames_per_file\n",
    "    #             print(numbers_all[i], traj_nr[i], file_number, file_number_frame)\n",
    "        root_new = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-{1}-protein/{0}-{1}-protein/{0}-{1}-protein'.format(test_system, traj_nr)\n",
    "        md_traj_start = md.load_dcd(root_new+'-{:03}.dcd'.format(file_number),\n",
    "                              top='/group/ag_cmb/scratch/deeptime_data/{0}/system-protein.pdb'.format(test_system)\n",
    "                        )\n",
    "        md_traj_small.xyz[i] = md_traj_start.xyz[file_number_frame]\n",
    "\n",
    "    md_traj_small.superpose(md_traj_super) \n",
    "    dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "\n",
    "    index_heavy_2nd, list_res = get_heavy_atoms_dssp(dssp)\n",
    "    print('Index of residues in 2nd structure', np.arange(n_residues)[list_res])\n",
    "    if index_heavy_2nd:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=index_heavy_2nd)\n",
    "        print('found secondary structure and aligning with {} atoms'.format(len(index_heavy_2nd)))\n",
    "    else:\n",
    "        md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=None)\n",
    "        print('did not find secondary structure and aligning with pdb {} atoms'.format(len(index_heavy_2nd)))\n",
    "\n",
    "    md_traj_small_temp.save_pdb('/group/ag_cmb/scratch/deeptime_data/{0}/attention/eig2_{1}_{2}_smooth{3}.pdb'.format(test_system, output_sizes[out_ind], o_temp, patchsize, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building hierarchical models with coarse-graining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cg(id, fac=1):\n",
    "    layer = Full_net.coarse_grain_layer[id]\n",
    "    \n",
    "    chi_X1_train = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "    chi_X2_train = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "\n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_X1_train, chi_X2_train)\n",
    "\n",
    "    matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "\n",
    "\n",
    "    for i in range(id):\n",
    "        chi_X1_train, chi_X2_train, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                chi_X1_train, chi_X2_train, u, S, u_t, renorm=Full_net.renorm)\n",
    "    K_test = K.detach().to('cpu').numpy().astype('float64')\n",
    "    T_test = K_test / K_test.sum(axis=1)[:, None]\n",
    "    from pyemma.msm import PCCA\n",
    "    test_a = PCCA(T_test, output_sizes[id+1])\n",
    "#     values = values / np.linalg.norm(values, axis=1, keepdims=True)\n",
    "    values = test_a.memberships\n",
    "    print(values)\n",
    "    values = np.log(values)*fac\n",
    "    with torch.no_grad():\n",
    "            \n",
    "            \n",
    "        layer.weight.copy_(torch.Tensor(values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cg = []\n",
    "for i in range(len(output_sizes)-1):\n",
    "    optimizer_cg.append(optim.Adam(Full_net.get_params_cg([i]), lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_net.coarse_grain_layer[0].reset_params()\n",
    "set_cg(0,1.)\n",
    "plot_cg(0)\n",
    "optimizer_cg[0] = optim.Adam(Full_net.get_params_cg([0]), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_before_rev_cg = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_before_rev_cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cg[0] = optim.Adam(Full_net.get_params_cg([0]), lr=0.1)\n",
    "# set_cg(0,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_cg_rev(0,2000,500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_cg_rev(0,3000,500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the reversible transition matrix at the coarse graining level\n",
    "id=0\n",
    "chi_X1_train = torch.Tensor(pred_batchwise(tensor_test_X1)).to(device)\n",
    "chi_X2_train = torch.Tensor(pred_batchwise(tensor_test_X2)).to(device)\n",
    "\n",
    "u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_X1_train, chi_X2_train)\n",
    "\n",
    "matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "\n",
    "\n",
    "for i in range(id):\n",
    "    chi_X1_train, chi_X2_train, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                    chi_X1_train, chi_X2_train, u, S, u_t, renorm=Full_net.renorm)\n",
    "\n",
    "chi_X1_train = chi_X1_train.detach()\n",
    "chi_X2_train = chi_X2_train.detach()\n",
    "u = u.detach()\n",
    "u_t = u_t.detach()\n",
    "S = S.detach()\n",
    "\n",
    "score_curr = vampe_loss_rev_cg(chi_X1_train, chi_X2_train, u, S, u_t, id, \n",
    "                                   return_mu=False, return_mu_K_Sigma=True, renorm=Full_net.renorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the eigenvalues of the coarse-grained model should recover the largest eigenvalues of K_msm\n",
    "torch.eig(score_curr[2]), print(np.linalg.eigvals(K_msm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for the second coarse-graining layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_net.coarse_grain_layer[1].reset_params()\n",
    "set_cg(1,1)\n",
    "plot_cg(1)\n",
    "# optimizer_cg[1] = optim.Adam(Full_net.get_params_cg([1]), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cg[1] = optim.Adam(Full_net.get_params_cg([1]), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_cg_rev(1,2000,500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_cg_rev(1,3000,500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=1\n",
    "chi_X1_train = torch.Tensor(pred_batchwise(tensor_test_X1)).to(device)\n",
    "chi_X2_train = torch.Tensor(pred_batchwise(tensor_test_X2)).to(device)\n",
    "\n",
    "u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_X1_train, chi_X2_train)\n",
    "\n",
    "matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "\n",
    "\n",
    "for i in range(id):\n",
    "    chi_X1_train, chi_X2_train, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                    chi_X1_train, chi_X2_train, u, S, u_t, renorm=Full_net.renorm)\n",
    "\n",
    "chi_X1_train = chi_X1_train.detach()\n",
    "chi_X2_train = chi_X2_train.detach()\n",
    "u = u.detach()\n",
    "u_t = u_t.detach()\n",
    "S = S.detach()\n",
    "\n",
    "score_curr = vampe_loss_rev_cg(chi_X1_train, chi_X2_train, u, S, u_t, id, \n",
    "                                   return_mu=False, return_mu_K_Sigma=True, renorm=Full_net.renorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eig(score_curr[2]), print(np.linalg.eigvals(K_msm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_after_cg = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the coarse graining matrices to optimize the score of all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_everything_rev(opt_list, runs, plot_mask_every=10, verbose=True, plot_training=True):\n",
    "    \n",
    "    chi_t_in = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "    chi_tau_in = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_6 = np.zeros(runs)\n",
    "    epoch_loss_3 = np.zeros(runs)\n",
    "    epoch_loss_2 = np.zeros(runs)\n",
    "    \n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "#         Full_net.\n",
    "#         opt = optimizer_full\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "#         Full_net.set_soft_fac(sen[[epoch]])\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "        running_epoch_loss_6 = []\n",
    "        running_epoch_loss_3 = []\n",
    "        running_epoch_loss_2 = []\n",
    "        \n",
    "\n",
    "            # zero the parameter gradients\n",
    "        chi_t = chi_t_in\n",
    "        chi_tau = chi_tau_in\n",
    "        \n",
    "        for opt in opt_list:\n",
    "            opt.zero_grad()\n",
    "        score_curr_list = []\n",
    "        \n",
    "        u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "\n",
    "        VampE_matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "        score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "        for j in range(len(output_sizes)-1):\n",
    "            chi_t, chi_tau, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[j].get_cg_uS(\n",
    "                                                                    chi_t, chi_tau, u, S, u_t, Full_net.renorm)\n",
    "            score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "\n",
    "        loss = - score_curr_list[0] - score_curr_list[1] - score_curr_list[2]\n",
    "\n",
    "        loss.backward()\n",
    "        for opt in opt_list:\n",
    "            opt.step()\n",
    "#             print(i, score_curr_list, inputs_t.shape)\n",
    "        running_epoch_loss.append(-loss.item())\n",
    "        running_epoch_loss_6.append(score_curr_list[0].item())\n",
    "        running_epoch_loss_3.append(score_curr_list[1].item())\n",
    "        running_epoch_loss_2.append(score_curr_list[2].item())\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        epoch_loss_6[epoch] = np.mean(running_epoch_loss_6)\n",
    "        epoch_loss_3[epoch] = np.mean(running_epoch_loss_3)\n",
    "        epoch_loss_2[epoch] = np.mean(running_epoch_loss_2)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, , 6 loss: {:.3}, , 3 loss: {:.3}, , 2 loss: {:.3}'.format(epoch+1, \n",
    "                                    epoch_loss[epoch], epoch_loss_6[epoch], epoch_loss_3[epoch], epoch_loss_2[epoch]))\n",
    "\n",
    "        if (((epoch+1) % plot_mask_every)==0):\n",
    "            plot_cg(0)\n",
    "            plot_cg(1)\n",
    "#         train_for_S(tensor_train_X1, tensor_train_X2, optimizer_rev_S, runs=1000, verbose=False, plot_training=True)\n",
    "\n",
    "#         train_for_u_S(tensor_train_X1, tensor_train_X2, optimizer_rev_u_S, runs=100, verbose=False, plot_training=True)\n",
    "        \n",
    "#     train_for_cg_rev(0, tensor_train_X1, tensor_train_X2, optimizer_cg[0], 1000, plot_mask_every=999,verbose=False,plot_training=True)\n",
    "#     train_for_cg_rev(1, tensor_train_X1, tensor_train_X2, optimizer_cg[1], 1000, plot_mask_every=999,verbose=False,plot_training=True)\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_6, label='VAMP_loss 6')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_3, label='VAMP_loss 3')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_2, label='VAMP_loss 2')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        \n",
    "def get_params_all_rev(coarse=False, u=False, S=False):\n",
    "    if coarse:\n",
    "        for layer in Full_net.coarse_grain_layer:\n",
    "            for param in layer.parameters():\n",
    "                yield param\n",
    "    if u:\n",
    "        for param in Full_net.u_layers[0].parameters():\n",
    "            yield param\n",
    "    if S:\n",
    "        for param in Full_net.S_layers[0].parameters():\n",
    "            yield param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = optim.Adam(get_params_all_rev(coarse=True), lr=learning_rate*100)\n",
    "opt2 = optim.Adam(get_params_all_rev(u=True), lr=learning_rate*10)\n",
    "opt3 = optim.Adam(get_params_all_rev(S=True), lr=learning_rate*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_temp = Full_net.get_weights()\n",
    "Full_net.set_weights(weights_after_cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_everything_rev([opt1, opt2, opt3], 1000, plot_mask_every=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_temp = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_final = Full_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_net.set_weights(weights_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to plot the hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coarse graining matrices\n",
    "list_weights = []\n",
    "for id in range(len(output_sizes)-1):\n",
    "    attention = Full_net.coarse_grain_layer[id].get_softmax()\n",
    "    attention_np = attention.detach().to('cpu').numpy()\n",
    "    list_weights.append(attention_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi(K):\n",
    "    eigvals, eigvec = np.linalg.eig(K.T)\n",
    "    \n",
    "    sort_id = np.argsort(eigvals)\n",
    "    eigvals = eigvals[sort_id]\n",
    "    eigvec = eigvec[:,sort_id]\n",
    "    pi = eigvec[:,-1]\n",
    "    pi = pi/pi.sum()\n",
    "    print(eigvals[-1], pi)\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K_rev_cg(id=0, tensor_t=torch.Tensor(traj_whole_new[0][:-tau]), tensor_tau=torch.Tensor(traj_whole_new[0][tau:])):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_t)).to(device)\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_tau)).to(device)\n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    \n",
    "    for i in range(id):\n",
    "        chi_t, chi_tau, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                        chi_t, chi_tau, u, S, u_t, Full_net.renorm)\n",
    "    \n",
    "    return K.detach().to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_rev_cg(id=0, tensor_t=torch.Tensor(traj_whole_new[0][:-tau]), tensor_tau=torch.Tensor(traj_whole_new[0][tau:])):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_t)).to(device)\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_tau)).to(device)\n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    \n",
    "    for i in range(id):\n",
    "        chi_t, chi_tau, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                        chi_t, chi_tau, u, S, u_t, Full_net.renorm)\n",
    "    \n",
    "    return mu_t.detach().to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_probs_rev_cg(id=0, tensor_t=torch.Tensor(traj_whole_new[0][:-tau]), tensor_tau=torch.Tensor(traj_whole_new[0][tau:])):\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_t)).to(device)\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_tau)).to(device)\n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    \n",
    "    for i in range(id):\n",
    "        chi_t, chi_tau, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[i].get_cg_uS(\n",
    "                                                                        chi_t, chi_tau, u, S, u_t, Full_net.renorm)\n",
    "    state_probs = torch.sum(chi_t * mu_t, dim=0)\n",
    "    return state_probs.detach().to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate all transition matrices\n",
    "K_6 = get_K_rev_cg(0)\n",
    "K_3 = get_K_rev_cg(1)\n",
    "K_2 = get_K_rev_cg(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the eigenvalues match\n",
    "np.linalg.eigvals(K_6), np.linalg.eigvals(K_3), np.linalg.eigvals(K_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the stationary distribution of each state\n",
    "pi = []\n",
    "pi.append(get_pi(K_6))\n",
    "pi.append(get_pi(K_3))\n",
    "pi.append(get_pi(K_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = []\n",
    "order_pi = True\n",
    "\n",
    "max_matrix = False\n",
    "fontsize_l = 36 \n",
    "fontsize_s = 32\n",
    "for t, coarse_matrix in enumerate(list_weights[::-1]):\n",
    "    if max_matrix:\n",
    "        vmax = coarse_matrix.max()\n",
    "    else:\n",
    "        vmax = 1.\n",
    "    if t == 0:\n",
    "#         order.append(np.arange(coarse_matrix.shape[1]))\n",
    "        order.append(np.argsort(pi[-1]))\n",
    "    sort_temp = order[-1]\n",
    "#     max_index = np.argmax(coarse_matrix[:,sort_temp], axis=1)\n",
    "#     sort_index = np.argsort(max_index)\n",
    "    sort_in = []\n",
    "    for index_out in sort_temp:\n",
    "        max_out_state = np.where(np.argmax(coarse_matrix, axis=1)==index_out)[0]\n",
    "        print(t, max_out_state)\n",
    "        print(pi[-2-t][max_out_state]*coarse_matrix[max_out_state,index_out])\n",
    "        if order_pi:\n",
    "            ratio_prob = pi[-2-t][max_out_state]*coarse_matrix[max_out_state,index_out]\n",
    "        else:\n",
    "            ratio_prob = coarse_matrix[max_out_state,index_out]\n",
    "        prob_sort = np.argsort(ratio_prob)\n",
    "        sort_in.append(max_out_state[prob_sort])\n",
    "    sort_index = np.concatenate(sort_in)\n",
    "    order.append(sort_index)\n",
    "    \n",
    "    plt.imshow((coarse_matrix[sort_index][:,sort_temp]).T**10, vmin=0., vmax=1, cmap=plt.cm.Reds)\n",
    "    plt.xticks(np.arange(coarse_matrix.shape[0]), fontsize=fontsize_s)\n",
    "    plt.yticks(np.arange(coarse_matrix.shape[1]), fontsize=fontsize_s)\n",
    "    plt.ylabel('To state', fontsize=fontsize_l)\n",
    "    plt.xlabel('From state', fontsize=fontsize_l)\n",
    "#     plt.savefig('./figs/matrix_{}to{}_{}_{}.svg'.format(coarse_matrix.shape[0], coarse_matrix.shape[1], patchsize, test), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stationary distribution for each model, they should be close\n",
    "mu_6 = get_mu_rev_cg(0)\n",
    "mu_3 = get_mu_rev_cg(1)\n",
    "mu_2 = get_mu_rev_cg(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_probs_6 = get_state_probs_rev_cg(0)\n",
    "state_probs_3 = get_state_probs_rev_cg(1)\n",
    "state_probs_2 = get_state_probs_rev_cg(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder them accordingly\n",
    "output_ave = []\n",
    "output_ave.append(get_pi(K_6)[order[-1]])\n",
    "output_ave.append(get_pi(K_3)[order[-2]])\n",
    "output_ave.append(get_pi(K_2)[order[-3]])\n",
    "print(output_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the coarse graining matrix aswell\n",
    "list_weights_new = []\n",
    "for t, weights in enumerate(list_weights):\n",
    "    \n",
    "    list_weights_new.append(weights[order[-t-1]][:,order[-t-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the names of the states\n",
    "names_6 = ['M','F','PF','U']\n",
    "names_3 = ['M', 'F', 'U']\n",
    "names_2 = ['M', 'U']\n",
    "names = [names_6, names_3, names_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes, activation, weights = None, max_plus_out = None, bias=None, fontsize=12, names=names):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    fontsize=28\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = ((top - bottom)/float(len(layer_sizes) - 1))/2\n",
    "    h_spacing = (right - left)/float(max(layer_sizes))\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        names_i = names[n]\n",
    "        if(max_plus_out is None):\n",
    "            max_plus = max(activation[n])\n",
    "        else:\n",
    "            max_plus = max_plus_out[n]\n",
    "        max_min = min(activation[n])\n",
    "        #print(max_plus)\n",
    "        if(max_plus == 0):\n",
    "            max_plus = 1.\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        left_layer =  (left + right)/2. - h_spacing*(layer_size - 1)/2.\n",
    "        for m in range(layer_size):\n",
    "            x = activation[n][m]#/max_plus\n",
    "            if(max_min >= 0):\n",
    "                r = 1\n",
    "#                 b = 1 - (x)**0.8\n",
    "#                 g = 1 - x**0.8\n",
    "                b = 1-(.25)**0.8\n",
    "                g = 1-(.25)**0.8\n",
    "            else:\n",
    "                if(activation[n][m] <0):\n",
    "                    r = activation[n][m]/max_min\n",
    "                    b = 0\n",
    "                    g = 0.25\n",
    "                else:\n",
    "                    r = 0\n",
    "                    b = x\n",
    "                    g = 0.25\n",
    "            radius = h_spacing/3. * x\n",
    "            circle = plt.Circle((left_layer + m*h_spacing, -n*v_spacing + top),radius,\n",
    "                                color=(r,g,b), ec='k', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "            names_new = names_i[m]\n",
    "            if x >= 0.1:\n",
    "                if len(names_new)<2:\n",
    "                    ax.text(left_layer + m*h_spacing -h_spacing/20.,-n*v_spacing + top-v_spacing/32., '{}'.format(names_new), zorder=10, fontsize=fontsize)\n",
    "                else:\n",
    "                    ax.text(left_layer + m*h_spacing -h_spacing/15.,-n*v_spacing + top-v_spacing/32., '{}'.format(names_new), zorder=10, fontsize=fontsize)\n",
    "            else:\n",
    "                ax.text(left_layer + m*h_spacing -h_spacing/8.,-n*v_spacing + top-v_spacing/32., '{}'.format(names_new), zorder=10, fontsize=fontsize)\n",
    "    # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        left_layer_a =  (left + right)/2. - h_spacing*(layer_size_a - 1)/2.\n",
    "        left_layer_b =  (left + right)/2. - h_spacing*(layer_size_b - 1)/2.\n",
    "        #print(layer_size_a)\n",
    "        #print(layer_size_b)\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                start_x = left_layer_a + m*h_spacing\n",
    "                end_x = left_layer_b + o*h_spacing\n",
    "                start_y = -n*v_spacing + top\n",
    "                end_y = -(n+1)*v_spacing + top\n",
    "                if(weights == None):\n",
    "                    line = plt.Line2D([start_x, end_x],\n",
    "                                      [start_y, end_y], c='k', label = 'line')\n",
    "                else:\n",
    "                    if(weights[n][m][o] > 0):\n",
    "                        c = 'k'\n",
    "                        ls = '-'\n",
    "                    else:\n",
    "                        c = 'grey'\n",
    "                        ls = '--'\n",
    "                    if weights[n][m][o] > 0.01:  \n",
    "#                     line = plt.Line2D([start_x , end_x],\n",
    "#                                       [start_y, end_y], c=c, label = 'line', linewidth = weights[n][m][o], linestyle = ls)\n",
    "                        if weights[n][m][o]>0.5:\n",
    "                            lw=1\n",
    "                        elif weights[n][m][o]>0.25:\n",
    "                            lw=0.5\n",
    "                        else:\n",
    "                            lw=0.25\n",
    "                        line = plt.Line2D([start_x , end_x],\n",
    "                                      [start_y, end_y], c=c, label = 'line', linewidth = lw, linestyle = ls)\n",
    "                diff_x = end_x - start_x\n",
    "                diff_y = end_y - start_y\n",
    "                fac = 1/5#*layer_size_b/2\n",
    "#                 fac2 = 1/layer_size_b\n",
    "                if weights[n][m][o] > 0.01:\n",
    "                    if weights[n][m][o] >= 0.999:\n",
    "                        ax.text(start_x + diff_x*fac + (m*o)*fac/256, -n*v_spacing + top - v_spacing*fac, '{:.0f}'.format(weights[n][m][o]*100), fontsize=fontsize)\n",
    "                    else:\n",
    "                        ax.text(start_x + diff_x*fac + (m*o)*fac/256, -n*v_spacing + top - v_spacing*fac, '{:.1f}'.format(weights[n][m][o]*100), fontsize=fontsize)\n",
    "                ax.add_artist(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "fontsize = 20\n",
    "draw_neural_net(ax, .1, .9, .1, .9, output_sizes, output_ave, weights=list_weights_new, max_plus_out=[1.]*len(output_sizes), fontsize=fontsize, names=names)\n",
    "# fig.savefig('./figs/coarse_grained_graph_{}_{}.svg'.format(patchsize, test), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save structures for each state of each model plus attention values as bfactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structures(cg_ind, vmax=1, skip=10, top=10):\n",
    "    n_residues = int(-1/2 + np.sqrt(1/4+input_size*2) + 3)\n",
    "    if Full_net.mask_const:\n",
    "        attention = Full_net.get_attention()\n",
    "        attention_np = attention.detach().numpy()\n",
    "        att_atom = np.reshape(attention_np, (n_residues,1))\n",
    "        plt.imshow(att_atom, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('System', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(1),['{}'.format(i) for i in range(1)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return att_atom\n",
    "        \n",
    "    else:\n",
    "        pred_temp = pred_batchwise(traj_whole_new[0], batchsize=10000)\n",
    "        pred_temp = torch.Tensor(pred_temp).to(device)\n",
    "        for i in range(cg_ind):\n",
    "            pred_temp = Full_net.forward_cg(pred_temp, i)\n",
    "        pred_temp = pred_temp.detach().to('cpu').numpy()\n",
    "        print(pred_temp.shape)\n",
    "        arg_sort = np.argsort(pred_temp, axis=0)\n",
    "        top_x_state = arg_sort[-top:][::-1]\n",
    "        states = pred_temp.shape[1]\n",
    "        att_atom = []\n",
    "        for state in range(states):\n",
    "            frames = top_x_state[:,state]\n",
    "            attention = Full_net.get_attention(torch.Tensor(traj_whole_new[0][frames]).to(device))\n",
    "            attention_np = attention.detach().to('cpu').numpy()\n",
    "#             att_atom.append(np.mean(attention_np, axis=0, keepdims=True))\n",
    "            print(attention_np.shape)\n",
    "            att_atom.append(attention_np[None,:,:])\n",
    "        att_residue = np.concatenate(att_atom, axis=0)\n",
    "        \n",
    "        plt.imshow(np.mean(att_residue,axis=1).T, vmin=0, vmax=vmax, aspect='auto')\n",
    "        plt.xlabel('State', fontsize=18)\n",
    "        plt.ylabel('Input', fontsize=18)\n",
    "        plt.xticks(np.arange(states),['{}'.format(i) for i in range(states)], fontsize=16)\n",
    "        plt.yticks(np.arange(0,n_residues,skip),['x{}'.format(i) for i in range(0,n_residues,skip)], fontsize=16)\n",
    "        plt.show()\n",
    "    #     plt.savefig('./Figs/2x3_mix_Mask.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return att_residue, top_x_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_nr=0\n",
    "threshold = 0\n",
    "for out_ind in range(len(output_sizes)):\n",
    "    attention, frames = structures(out_ind, top=number_of_frames, vmax=3)\n",
    "    for o_temp in range(frames.shape[1]):\n",
    "        order_o = order[-out_ind-1]\n",
    "        o = order_o[o_temp]\n",
    "        print(o)\n",
    "        attention_clean = np.zeros_like(attention[o])\n",
    "        for f in range(number_of_frames):\n",
    "            for i in range(n_residues):\n",
    "                if attention[o,f,i] < threshold:\n",
    "                    attention_clean[f,i]=0\n",
    "                else:\n",
    "                    attention_clean[f,i]=attention[o,f,i] \n",
    "        attention_fixed = np.concatenate([attention_clean, attention_clean], axis=1)\n",
    "        bfactors = np.repeat(attention_fixed, [res.n_atoms for res in md_traj_super.top.residues], axis=1)\n",
    "\n",
    "        frames_of_state = frames[:,o]\n",
    "        frames_per_file = 10000\n",
    "        for i in range(number_of_frames):\n",
    "            file_number = frames_of_state[i]//frames_per_file\n",
    "            file_number_frame = frames_of_state[i]%frames_per_file\n",
    "        #             print(numbers_all[i], traj_nr[i], file_number, file_number_frame)\n",
    "            root_new = '/group/ag_cmb/simulation-data/DESRES-Science2011-FastProteinFolding/DESRES-Trajectory_{0}-{1}-protein/{0}-{1}-protein/{0}-{1}-protein'.format(test_system, traj_nr)\n",
    "            md_traj_start = md.load_dcd(root_new+'-{:03}.dcd'.format(file_number),\n",
    "                                  top='/group/ag_cmb/scratch/deeptime_data/{0}/system-protein.pdb'.format(test_system)\n",
    "                            )\n",
    "            md_traj_small.xyz[i] = md_traj_start.xyz[file_number_frame]\n",
    "\n",
    "        md_traj_small.superpose(md_traj_super) \n",
    "        dssp = md.compute_dssp(md_traj_small, simplified=True)[:,:n_residues]\n",
    "\n",
    "        index_heavy_2nd, list_res = get_heavy_atoms_dssp(dssp)\n",
    "        print('Index of residues in 2nd structure', np.arange(n_residues)[list_res])\n",
    "        if index_heavy_2nd:\n",
    "            md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=index_heavy_2nd)\n",
    "            print('found secondary structure and aligning with {} atoms'.format(len(index_heavy_2nd)))\n",
    "        else:\n",
    "            md_traj_small_temp = md_traj_small.superpose(md_traj_small, frame=0, atom_indices=None)\n",
    "            print('did not find secondary structure and aligning with pdb {} atoms'.format(len(index_heavy_2nd)))\n",
    "        \n",
    "        md_traj_small_temp.save_pdb('/group/ag_cmb/scratch/deeptime_data/{0}/attention/new_{1}_{2}_smooth{3}_{4}.pdb'.format(test_system, output_sizes[out_ind], o_temp, patchsize, test), bfactors=bfactors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timescale calculation for the coarse graining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_all_rev():\n",
    "        \n",
    "    for layer in Full_net.coarse_grain_layer:\n",
    "        for param in layer.parameters():\n",
    "            yield param\n",
    "\n",
    "    for param in Full_net.u_layers[0].parameters():\n",
    "        yield param\n",
    "    \n",
    "    for param in Full_net.S_layers[0].parameters():\n",
    "        yield param\n",
    "        \n",
    "def vampe_loss_rev(chi_t, chi_tau, layer_id=0, return_mu=False, return_mu_K_Sigma=False):\n",
    "    \n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[layer_id](chi_t, chi_tau)\n",
    "    matrix, K, _ = Full_net.S_layers[layer_id](v, C_00, C_11, C_01, Sigma)\n",
    "    vampe = torch.trace(matrix)\n",
    "    \n",
    "    if return_mu:\n",
    "        return -vampe, mu_t\n",
    "    \n",
    "    elif return_mu_K_Sigma:\n",
    "        return -vampe, mu_t, K, Sigma_t\n",
    "    \n",
    "    else:\n",
    "        return -vampe\n",
    "    \n",
    "def vampe_loss_rev_only_S(v, C_00, C_11, C_01, Sigma, layer_id=0):\n",
    "    \n",
    "    matrix, K, _ = Full_net.S_layers[layer_id](v, C_00, C_11, C_01, Sigma)\n",
    "#     print(K)\n",
    "    vampe = torch.trace(matrix)\n",
    "    \n",
    "    return -vampe\n",
    "\n",
    "def vampe_loss_rev_cg(chi_t, chi_tau, u, S, u_t, layer_id=0, return_mu=False, return_mu_K_Sigma=False, renorm=True):\n",
    "    \n",
    "    \n",
    "    # only this line should be the part of it\n",
    "    chi_t_m, chi_tau_m, u_m, u_t_m, S_m, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[layer_id].get_cg_uS(\n",
    "                                                                        chi_t, chi_tau, u, S, u_t, renorm)\n",
    "    \n",
    "    vampe = torch.trace(VampE_matrix)\n",
    "    \n",
    "    if return_mu:\n",
    "        return -vampe, mu_t\n",
    "    \n",
    "    elif return_mu_K_Sigma:\n",
    "        return -vampe, mu_t, K, Sigma_t\n",
    "    \n",
    "    else:\n",
    "        return -vampe\n",
    "    \n",
    "def train_for_S(chi_t, chi_tau, opt, runs=100, verbose=True, plot_training=True):\n",
    "    \n",
    "#     chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device)\n",
    "#     chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device)\n",
    "    _, _, v, C_00, C_11, C_01, Sigma, _, _ = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "    \n",
    "    v = v.detach()\n",
    "    C_00 = C_00.detach()\n",
    "    C_11 = C_11.detach()\n",
    "    C_01 = C_01.detach()\n",
    "    Sigma = Sigma.detach()\n",
    "    chi_t.detach()\n",
    "    chi_tau.detach()\n",
    "    \n",
    "    chi_t_valid = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device)\n",
    "    chi_tau_valid = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device)\n",
    "    _, _, v_valid, C_00_valid, C_11_valid, C_01_valid, Sigma_valid, _, _ = Full_net.u_layers[0](chi_t_valid, chi_tau_valid)\n",
    "    \n",
    "    v_valid = v_valid.detach()\n",
    "    C_00_valid = C_00_valid.detach()\n",
    "    C_11_valid = C_11_valid.detach()\n",
    "    C_01_valid = C_01_valid.detach()\n",
    "    Sigma_valid = Sigma_valid.detach()\n",
    "    chi_t_valid.detach()\n",
    "    chi_tau_valid.detach()\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "#     opt = optimizer_rev_S\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "\n",
    "#             print(chi_t)\n",
    "\n",
    "        score_curr = vampe_loss_rev_only_S(v, C_00, C_11, C_01, Sigma)\n",
    "\n",
    "\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(-loss.item())\n",
    "        \n",
    "        score_curr_valid = vampe_loss_rev_only_S(v_valid, C_00_valid, C_11_valid, C_01_valid, Sigma_valid)\n",
    "        epoch_loss_valid[epoch] = np.mean(score_curr_valid.item())\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "def train_for_u_S(chi_t, chi_tau, opt, runs=100, verbose=True, plot_training=True):\n",
    "    \n",
    "#     chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device).detach()\n",
    "#     chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device).detach()\n",
    "    \n",
    "    chi_t_valid = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "    chi_tau_valid = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "#     opt = optimizer_rev_u_S\n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # estimate weights\n",
    "#             print(inputs_t)\n",
    "\n",
    "#             print(chi_t)\n",
    "        score_curr = vampe_loss_rev(chi_t, chi_tau)\n",
    "\n",
    "        loss = - score_curr\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        score_curr_valid = vampe_loss_rev(chi_t_valid, chi_tau_valid)\n",
    "        epoch_loss[epoch] = np.mean(-loss.item())\n",
    "        epoch_loss_valid[epoch] = np.mean(score_curr_valid.item())\n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, valid loss: {:.3}'.format(epoch+1, epoch_loss[epoch], epoch_loss_valid[epoch]))\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_valid, label='VAMP_loss_valid')\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "\n",
    "\n",
    "def train_for_everything_rev(chi_t_in, chi_tau_in, opt, runs, plot_mask_every=10, verbose=True, plot_training=True):\n",
    "    \n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_valid = np.zeros(runs)\n",
    "    \n",
    "    epoch_loss = np.zeros(runs)\n",
    "    epoch_loss_6 = np.zeros(runs)\n",
    "    epoch_loss_3 = np.zeros(runs)\n",
    "    epoch_loss_2 = np.zeros(runs)\n",
    "    \n",
    "    for epoch in range(runs):  # loop over the dataset multiple times\n",
    "\n",
    "#         Full_net.\n",
    "        opt = opt\n",
    "    #     sen_temp = sen_set[epoch//sen_every]\n",
    "\n",
    "#         Full_net.set_soft_fac(sen[[epoch]])\n",
    "\n",
    "\n",
    "        running_epoch_loss = []\n",
    "        running_epoch_loss_6 = []\n",
    "        running_epoch_loss_3 = []\n",
    "        running_epoch_loss_2 = []\n",
    "        \n",
    "\n",
    "            # zero the parameter gradients\n",
    "        chi_t = chi_t_in\n",
    "        chi_tau = chi_tau_in\n",
    "        opt.zero_grad()\n",
    "        score_curr_list = []\n",
    "        \n",
    "        u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t, chi_tau)\n",
    "\n",
    "        VampE_matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "        score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "        for j in range(len(output_sizes)-1):\n",
    "            chi_t, chi_tau, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[j].get_cg_uS(\n",
    "                                                                    chi_t, chi_tau, u, S, u_t, Full_net.renorm)\n",
    "            score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "\n",
    "        loss = - score_curr_list[0] - score_curr_list[1] - score_curr_list[2]\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "#             print(i, score_curr_list, inputs_t.shape)\n",
    "        running_epoch_loss.append(-loss.item())\n",
    "        running_epoch_loss_6.append(score_curr_list[0].item())\n",
    "        running_epoch_loss_3.append(score_curr_list[1].item())\n",
    "        running_epoch_loss_2.append(score_curr_list[2].item())\n",
    "\n",
    "\n",
    "        epoch_loss[epoch] = np.mean(running_epoch_loss)\n",
    "        epoch_loss_6[epoch] = np.mean(running_epoch_loss_6)\n",
    "        epoch_loss_3[epoch] = np.mean(running_epoch_loss_3)\n",
    "        epoch_loss_2[epoch] = np.mean(running_epoch_loss_2)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Run {}, total loss: {:.3}, , 6 loss: {:.3}, , 3 loss: {:.3}, , 2 loss: {:.3}'.format(epoch+1, \n",
    "                                    epoch_loss[epoch], epoch_loss_6[epoch], epoch_loss_3[epoch], epoch_loss_2[epoch]))\n",
    "\n",
    "        if (((epoch+1) % plot_mask_every)==0):\n",
    "            plot_cg(0)\n",
    "            plot_cg(1)\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss, label='VAMP_loss')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_6, label='VAMP_loss 6')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_3, label='VAMP_loss 3')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "        plt.plot(np.arange(1,runs+1), epoch_loss_2, label='VAMP_loss 2')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "\n",
    "def get_Ks_for_tau(tau_i):\n",
    "    X1_train_cor, X2_train_cor, X1_vali_cor, X2_vali_cor, X1_test_cor, X2_test_cor, length_train, length_vali, _, _ = get_data_for_tau(traj_whole_new, tau_i)\n",
    "\n",
    "\n",
    "    tensor_train_X1 = torch.Tensor(X1_train_cor)\n",
    "    tensor_train_X2 = torch.Tensor(X2_train_cor) # transform to torch tensor\n",
    "    tensor_valid_X1 = torch.Tensor(X1_vali_cor)\n",
    "    tensor_valid_X2 = torch.Tensor(X2_vali_cor)\n",
    "    tensor_test_X1 = torch.Tensor(X1_test_cor)\n",
    "    tensor_test_X2 = torch.Tensor(X2_test_cor)\n",
    "\n",
    "    trainset = data.TensorDataset(tensor_train_X1, tensor_train_X2) # create your datset\n",
    "    trainloader = data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                  shuffle=True, num_workers=2)\n",
    "    Full_net.set_weights(weights_final)\n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "    score_old = vampe_loss_rev(chi_t, chi_tau).item()\n",
    "    \n",
    "    print(score_old)\n",
    "    \n",
    "    weights_temp = Full_net.get_weights()\n",
    "    with torch.no_grad():\n",
    "        for param in Full_net.S_layers[0].parameters():\n",
    "            param.copy_(torch.Tensor(np.ones((output_sizes[0], output_sizes[0]))))\n",
    "    with torch.no_grad():\n",
    "        for param in Full_net.u_layers[0].parameters():\n",
    "            param.copy_(torch.Tensor(np.ones((1, output_sizes[0]))))\n",
    "            \n",
    "    optimizer_rev_u_S_cg = optim.Adam(Full_net.get_params_rev(all=False), lr=learning_rate/10)\n",
    "    optimizer_rev_S_cg = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*10)\n",
    "    optimizer_rev_u_S = optim.Adam(Full_net.get_params_rev(all=False), lr=learning_rate/10)\n",
    "    optimizer_rev_S = optim.Adam(Full_net.get_params_rev(all=False, u_flag=False), lr=learning_rate*10)\n",
    "    \n",
    "#     Full_net.coarse_grain_layer[0].reset_params()\n",
    "#     optimizer_cg[0] = optim.Adam(Full_net.get_params_cg([0]), lr=0.1)\n",
    "#     Full_net.coarse_grain_layer[1].reset_params()\n",
    "#     optimizer_cg[1] = optim.Adam(Full_net.get_params_cg([1]), lr=0.1)\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_valid_X1)).to(device).detach()\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_valid_X2)).to(device).detach()\n",
    "    score_old = vampe_loss_rev(chi_t, chi_tau).item()\n",
    "    \n",
    "    print(score_old)\n",
    "    \n",
    "    chi_t = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device).detach()\n",
    "    chi_tau = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device).detach()\n",
    "            \n",
    "    flag=True\n",
    "    counter = 0\n",
    "    score_curr_list=[]\n",
    "    chi_t_v = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device).detach()\n",
    "    chi_tau_v = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device).detach()\n",
    "    u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t_v, chi_tau_v)\n",
    "\n",
    "    VampE_matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "    score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "    for j in range(len(output_sizes)-1):\n",
    "        chi_t_v, chi_tau_v, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[j].get_cg_uS(\n",
    "                                                                chi_t_v, chi_tau_v, u, S, u_t, Full_net.renorm)\n",
    "        score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "\n",
    "    loss = - score_curr_list[0] - score_curr_list[1] - score_curr_list[2]\n",
    "    score_old = - loss.item()\n",
    "    print('Old score', score_old)\n",
    "    while flag:\n",
    "        score_curr_list = []\n",
    "#         train_for_S(chi_t, chi_tau, optimizer_rev_S, runs=1000, verbose=False, plot_training=False)\n",
    "\n",
    "#         train_for_u_S(chi_t, chi_tau, optimizer_rev_u_S, runs=1000, verbose=False, plot_training=False)\n",
    "        train_for_everything_rev(chi_t, chi_tau, optimizer_rev_S_cg, runs=250,plot_mask_every=5000, verbose=False, plot_training=False)\n",
    "        train_for_everything_rev(chi_t, chi_tau, optimizer_rev_u_S_cg, runs=250,plot_mask_every=5000, verbose=False, plot_training=False)\n",
    "        if counter > 50:\n",
    "            flag=False\n",
    "        counter +=1\n",
    "        chi_t_v = torch.Tensor(pred_batchwise(tensor_train_X1)).to(device).detach()\n",
    "        chi_tau_v = torch.Tensor(pred_batchwise(tensor_train_X2)).to(device).detach()\n",
    "        u, u_t, v, C_00, C_11, C_01, Sigma, mu_t, Sigma_t = Full_net.u_layers[0](chi_t_v, chi_tau_v)\n",
    "\n",
    "        VampE_matrix, K, S = Full_net.S_layers[0](v, C_00, C_11, C_01, Sigma)\n",
    "        score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "        for j in range(len(output_sizes)-1):\n",
    "            chi_t_v, chi_tau_v, u, u_t, S, mu_t, Sigma_t, K, VampE_matrix = Full_net.coarse_grain_layer[j].get_cg_uS(\n",
    "                                                                    chi_t_v, chi_tau_v, u, S, u_t, Full_net.renorm)\n",
    "            score_curr_list.append(-torch.trace(VampE_matrix))\n",
    "\n",
    "        loss = - score_curr_list[0] - score_curr_list[1] - score_curr_list[2]\n",
    "        score = - loss.item()\n",
    "        print(score)\n",
    "        if score > score_old:\n",
    "            weights_temp = Full_net.get_weights()\n",
    "            score_old = score\n",
    "#             print(score)\n",
    "        else:\n",
    "            flag=False\n",
    "            Full_net.set_weights(weights_temp)\n",
    "            print('Score decreased')\n",
    "    \n",
    "\n",
    "\n",
    "    K_list = []\n",
    "    \n",
    "    for o in range(len(output_sizes)):\n",
    "        K_list.append(get_K_rev_cg(o, tensor_t=tensor_train_X1, tensor_tau=tensor_train_X2))\n",
    "        print(np.linalg.eigvals(K_list[-1]))\n",
    "    return K_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_size = tau*skip\n",
    "max_tau = 10000\n",
    "# lag = np.arange(50, max_tau, step_size)\n",
    "# lag = (np.linspace(3.22,6.3, 10)**4).astype('int')\n",
    "print(lag)\n",
    "K_results_6 = np.ones((len(lag) ,output_sizes[0], output_sizes[0]))\n",
    "K_results_3 = np.ones((len(lag) ,output_sizes[1], output_sizes[1]))\n",
    "K_results_2 = np.ones((len(lag) ,output_sizes[2], output_sizes[2]))\n",
    "for i, tau_i in enumerate(lag):\n",
    "    print(tau_i)\n",
    "#     K_results_rev[i]= training_for_tau_both(tau_i)\n",
    "    K_list = get_Ks_for_tau(tau_i)\n",
    "    K_results_6[i]  = K_list[0]\n",
    "    K_results_3[i]  = K_list[1]\n",
    "    K_results_2[i]  = K_list[2]\n",
    "\n",
    "its_6 = get_its(K_results_6, lag, False)\n",
    "its_3 = get_its(K_results_3, lag, False)\n",
    "its_2 = get_its(K_results_2, lag, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot timescales for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 200.*skip*1e-6 \n",
    "plt.figure(figsize=(6,4));\n",
    "\n",
    "label_x = np.array([.1,0.3,1, 2, 5,10,100,1000])/fac # array is in microsecond\n",
    "label_y = np.array([.1,1, 2, 5,10, 100, 1000])/fac\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "for j in range(0,output_sizes[0]-1):\n",
    "    plt.semilogy(lag, its_6[::-1][j], lw=5)\n",
    "#     plt.fill_between(tau_list, all_its_mean[i] -all_its_std[i], all_its_mean[i] + all_its_std[i], alpha = 0.3)\n",
    "plt.semilogy(lag,lag, 'k')\n",
    "plt.xlabel('lag [$\\mu$s]', fontsize=26)\n",
    "plt.xticks(label_x, label_x*fac, fontsize=22)\n",
    "plt.ylabel('timescale [$\\mu$s]', fontsize=26)\n",
    "plt.yticks(label_y, np.round(label_y*fac, decimals=1), fontsize=22)\n",
    "plt.fill_between(lag,lag,0.1,alpha = 0.2,color='k');\n",
    "plt.ylim(0.01/fac, 3/fac)\n",
    "plt.xlim(lag[0], 0.3/fac)\n",
    "# plt.title('Villin', fontsize=16)\n",
    "# plt.savefig('figs/its_{}_{}_{}.pdf'.format(output_sizes[0], patchsize, test), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 200.*skip*1e-6 \n",
    "plt.figure(figsize=(6,4));\n",
    "\n",
    "label_x = np.array([.1,0.3,1, 2, 5,10,100,1000])/fac # array is in microsecond\n",
    "label_y = np.array([.1,1, 2, 5,10, 100, 1000])/fac\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "for j in range(0,output_sizes[1]-1):\n",
    "    plt.semilogy(lag, its_3[::-1][j], lw=5)\n",
    "#     plt.fill_between(tau_list, all_its_mean[i] -all_its_std[i], all_its_mean[i] + all_its_std[i], alpha = 0.3)\n",
    "plt.semilogy(lag,lag, 'k')\n",
    "plt.xlabel('lag [$\\mu$s]', fontsize=26)\n",
    "plt.xticks(label_x, label_x*fac, fontsize=22)\n",
    "plt.ylabel('timescale [$\\mu$s]', fontsize=26)\n",
    "plt.yticks(label_y, np.round(label_y*fac, decimals=1), fontsize=22)\n",
    "plt.fill_between(lag,lag,0.1,alpha = 0.2,color='k');\n",
    "plt.ylim(0.01/fac, 3/fac)\n",
    "plt.xlim(lag[0], 0.3/fac)\n",
    "# plt.title('Villin', fontsize=16)\n",
    "# plt.savefig('figs/its_{}_{}_{}.pdf'.format(output_sizes[1], patchsize, test), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 200.*skip*1e-6 \n",
    "plt.figure(figsize=(6,4));\n",
    "\n",
    "label_x = np.array([.1,0.3,1, 2, 5,10,100,1000])/fac # array is in microsecond\n",
    "label_y = np.array([.1,1, 2, 5,10, 100, 1000])/fac\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "for j in range(0,output_sizes[2]-1):\n",
    "    plt.semilogy(lag, its_2[::-1][j], lw=5)\n",
    "#     plt.fill_between(tau_list, all_its_mean[i] -all_its_std[i], all_its_mean[i] + all_its_std[i], alpha = 0.3)\n",
    "plt.semilogy(lag,lag, 'k')\n",
    "plt.xlabel('lag [$\\mu$s]', fontsize=26)\n",
    "plt.xticks(label_x, label_x*fac, fontsize=22)\n",
    "plt.ylabel('timescale [$\\mu$s]', fontsize=26)\n",
    "plt.yticks(label_y, np.round(label_y*fac, decimals=1), fontsize=22)\n",
    "plt.fill_between(lag,lag,0.1,alpha = 0.2,color='k');\n",
    "plt.ylim(0.01/fac, 3/fac)\n",
    "plt.xlim(lag[0], 0.3/fac)\n",
    "# plt.title('Villin', fontsize=16)\n",
    "# plt.savefig('figs/its_{}_{}_{}.pdf'.format(output_sizes[2], patchsize, test), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
